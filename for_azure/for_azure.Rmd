---
title: "Synapse Workflow"
author: "ROSSyndicate"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  markdown: 
    wrap: 90
---

This workflow houses the PWQN QAQC workflow. In order to run this pipeline, you 
must have the following additional data sets:
1. Field notes
2. HydroVu credentials yaml
3. mWater credentials yaml

1. Import and collate data
  a. Import historical field notes via excel sheet. Import field data via mWater API.
  b. Collate field data.
  c. Import sonde data via HydroVu API. 
  d. Collate sonde data.
2. Develop site-parameter data thresholds
  a. Join Sonde data with field notes
  b. Add summary statistics
  c. Define thresholds
3. Flag
  a. Single sensor flags
  b. Network check

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warnings = 'hide', error = FALSE, message = 'hide')
```

```{r}
source("for_azure/R/package_loader.R")
lapply(c("data.table", "tidyverse", "rvest", "readxl", "lubridate", "zoo", "padr",
         "plotly", "feather", "RcppRoll", "yaml", "ggpubr", "profvis", "janitor", 
         "HydroVuR", "here"), package_loader)
# have to find these in here: https://cran.r-project.org/web/packages/available_packages_by_name.html

# Load functions
walk(list.files('for_azure/R/', pattern = "*.R", full.names = TRUE, recursive = TRUE), source)
```

## *Step 1: Import and collate data*

*Load field notes and define the start time as the 15 minutes preceding the recorded field
time*

```{r field notes}
old_field_notes <- read_csv(here("for_azure", "qaqc", "old_field_notes.csv"))

mWater_field_notes <- load_mWater_notes(creds = yaml::read_yaml(here("creds","mWaterCreds.yml"))) 

new_field_notes <- mWater_field_notes %>% 
  grab_mWater_sensor_notes(mWater_api_data = .)

#merge new mwater notes (sensor_notes) and old notes (field notes)  
all_field_notes <- rbind(old_field_notes, new_field_notes) %>%
  mutate(site = ifelse(site == "riverbluffs", "river bluffs", site))

sensor_malfunction_notes <- mWater_field_notes %>% 
  grab_mWater_malfunction_notes(mWater_api_data = .)
```

*Merge the data sets from all API pulls. Then for developing this workflow, subset the
data to only the 2022 and 2023 field season. (Previous field seasons were managed quite
differently, and therefore should be treated differently.)*

```{r}
hv_creds <- read_yaml(yaml::as.yaml(here("creds","HydroVuCreds.yml")))

hv_token <- hv_auth(client_id = as.character(hv_creds["client"]),
                    client_secret = as.character(hv_creds["secret"]),
                    url = "https://www.hydrovu.com/public-api/oauth/token")

incoming_data_csvs_upload <- api_puller(site = c("Tamasag", "Legacy", "Lincoln",
                                                 "Timberline","Prospect", "Boxelder",
                                                 "Archery", "River Bluffs"),
                                        start_dt = "2023-01-01 01:00:00 MDT",
                                        end_dt = Sys.time(),
                                        api_token = hv_token,
                                        dump_dir = "data/api/virridy/",
                                        require = NULL)

# we will use `munge_API_data()` for the synapse workflow
incoming_data_collated_csvs <- munge_api_data(api_path = "data/api/virridy/",
                                              network = "virridy",
                                              require = NULL)
all_data <- incoming_data_collated_csvs

```

## *Step 2: Develop site-parameter data thresholds*

*Here, we split up all of our site-parameter combinations into a list that we can more
easily iterate over. Then, across those lists, we average any observations whose frequency
is greater than 15 minutes so that our data set is consistently recorded at 15-minute
intervals. We also preserve the total number of observations within the 15-minute
increment used to calculate the mean, as well as the spread (max-min). After these
calculations, we use {padr}'s `pad()` function to fill in data gaps at this 15-minute
interval. Lastly, we join these data frames with the field notes.*

```{r}
# format and summarize data
# Determine each site and parameter in all_data 
sites <- unique(all_data$site)
params <- c(
  "Chl-a Fluorescence", 
  "Depth", 
  "DO", 
  "ORP", 
  "pH",
  "Specific Conductivity",
  "Temperature",
  "Turbidity",
  "FDOM Fluorescence")

# Constructing a df to iterate over each site-parameter combination
site_param_combos <- tidyr::crossing(sites, params)

# Make a list of the 15-minute summarized data, joined with field notes
all_data_tidied_list <- purrr::map2(
  .x = site_param_combos$sites,
  .y = site_param_combos$params,
  ~ tidy_and_add_field_notes(
      site_arg = .x,
      parameter_arg = .y,
      api_data = all_data,
      notes = all_field_notes,
      summarize_interval = "15 minutes") %>%
    # set the names for the dfs in the list
    purrr::set_names(paste0(site_param_combos$sites, "-", site_param_combos$params)) %>%
    # remove NULL from the list (indicating a site-param combo that doesn't actually exist)
    purrr::keep( ~ !is.null(.))
)
```

#### *Add summary stats*

Here, we are adding in contextual summary statistics that can be used to describe a given
observation's relationship to its neighboring observations. This includes:

-   *the previous and next observation and their slopes*
-   *the 7-point (each observation and the previous 6) moving median, mean, slope, and
    standard deviation*
-   *the hydrologic "season" in which the observation lands in*: 
    - Winter base flow: Dec, Jan, Feb, Mar, Apr 
    - Snow melt: May, Jun 
    - Monsoon: Jul, Aug, Sep 
    - Fall base flow: Oct, Nov

```{r}
all_data_summary_stats_list <- all_data_tidied_list %>%
  # modified generate_summary_statistics (for performing across "full" dataset)
  purrr::map(~ generate_summary_statistics(.)) 
```

#### *Define thresholds*

*Next, we create a look-up table for site-parameter thresholds to use in flagging strange
data. These thresholds will ULTIMATELY be based on the approved subset of human-verified GOOD data, and will be fixed (i.e., unchanging). But for now, we can reduce the full dataset to "GOOD"-ish data by using some automated cleaning steps as a placeholder. 

```{r}
include_verified <- FALSE

if (include_verified == TRUE){
  
  verified_data <- list.files("data/virridy_verification/verified_directory/", full.names = TRUE) %>%
    map_dfr(~readRDS(.) %>% dplyr::mutate(verification_status = as.character(verification_status))) %>%
    data.table::data.table() %>%
    dplyr::mutate(date = as_date(DT_round),
                  clean_mean = case_when(is.na(flag) & verification_status == "PASS" ~ mean,
                                         is.na(flag) & verification_status == "FAIL" ~ NA,
                                         !is.na(flag) & verification_status == "PASS" ~ NA,
                                         !is.na(flag) & verification_status == "FAIL" ~ mean),
                  clean_flag = case_when(is.na(flag) & verification_status == "PASS" ~ NA,
                                         is.na(flag) & verification_status == "FAIL" ~ "needs a flag",
                                         !is.na(flag) & verification_status == "PASS" ~ flag,
                                         !is.na(flag) & verification_status == "FAIL" ~ NA)) %>%
    dplyr::select(DT_join, site, parameter, clean_mean, verification_status) #%>%
  # reconfigure to site-parameter for future processing
  #split(f = list(.$site, .$parameter), sep = "-") 
  
  ver_names <- str_replace(list.files("data/virridy_verification/verified_directory/"), ".RDS", "")
  
  goodish_data_placeholder <- purrr::map(all_data_summary_stats_list, function(data){
    # single-sensor flags
    data %>%
      # flag field visits
      add_field_flag(df = .) %>%
      # incorporate post-corrected values for development of thresholds:
      # This function is still very much a WIP!
      fix_calibration(df = ., cal_errors = readxl::read_excel("data/calibration_error_log.xlsx")) %>%
      # flag instances of known sensor malfunction
      add_malfunction_flag(df = ., malfunction_records = sensor_malfunction_notes) %>%
      # flag instances outside the spec range
      add_spec_flag(df = ., spec_table = yaml::read_yaml("data/qaqc/sensor_spec_thresholds.yml")) %>%
      # find times when sonde was moved up/down in housing (placeholder function, needs enhancements!)
      add_depth_shift_flag(df = ., level_shift_table =  readr::read_csv('data/qaqc/level_shifts.csv')) 
  }) %>%
    dplyr::bind_rows() %>%
    #intersensor flags require data by SITE, not SITE-PARAMETER
    split(f = .$site, sep = "-") %>%
    purrr::map(., function(data){
      data %>%
        # flag data when water was below freezing
        add_frozen_flag(df = .) %>%
        # flag data when sonde was not submerged in water
        add_unsubmerged_flag() %>%
        # Filter to only un-flagged data, and data that was collected when sonde
        # was actually deployed
        dplyr::filter(is.na(flag) | sonde_employed != 1) %>%
        #... and when the sonde wasn't moved
        dplyr::filter(is.na(depth_change) | depth_change != "sonde moved")
    }) %>% 
    dplyr::bind_rows() %>%
    # reconfigure to site-parameter for future processing
    split(f = list(.$site, .$parameter), sep = "-") %>%
    # remove NULL from the list (indicating a site-param combo that doesn't actually exist)
    purrr::keep(~ !is.null(.))
  
  goodish_data_placeholder <- goodish_data_placeholder[!names(goodish_data_placeholder) %in% ver_names] %>%
    bind_rows()
  
  verified_data <- all_data_summary_stats_list %>%
    bind_rows() %>%
    data.table() %>%
    dplyr::right_join(., verified_data, by = c("DT_join", "site", "parameter")) %>%
    dplyr::filter(!is.na(clean_mean)) %>%
    dplyr::select(-c(clean_mean, verification_status)) 
  
  combined_clean_data <- bind_rows(verified_data, goodish_data_placeholder) %>%
    split(f = list(.$site, .$parameter), sep = "-") %>%
    # remove NULL from the list (indicating a site-param combo that doesn't actually exist)
    purrr::keep(~ nrow(.) != 0)
  
  missing <- combined_clean_data[!names(combined_clean_data) %in% names(all_data_summary_stats_list)] 
  
  threshold_lookup <- purrr::map(combined_clean_data, make_threshold_table) %>%
    dplyr::bind_rows()
  old_thresholds <- read_csv("data/qaqc/seasonal_thresholds.csv")
  readr::write_csv(threshold_lookup, 'data/qaqc/seasonal_thresholds.csv')
  
  
} else {
  
  goodish_data_placeholder <- purrr::map(all_data_summary_stats_list, function(data){
    #single-sensor flags
    data %>%
      # flag field visits
      add_field_flag(df = .) %>%
      # Replace any NTU > 1000 with 1000,
      # add "raw" data column ahead of modifying any `mean` values:
      fix_turbidity(df = .) %>%
      # incorporate post-corrected values for development of thresholds:
      # This function is still very much a WIP!
      fix_calibration(df = ., cal_errors = readxl::read_excel("data/calibration_error_log.xlsx")) %>%
      # flag instances of known sensor malfunction
      add_malfunction_flag(df = ., malfunction_records = sensor_malfunction_notes) %>%
      # flag instances outside the spec range
      add_spec_flag(df = ., spec_table = yaml::read_yaml("data/qaqc/sensor_spec_thresholds.yml")) %>%
      # find times when sonde was moved up/down in housing (placeholder function, needs enhancements!)
      add_depth_shift_flag(df = ., level_shift_table =  readr::read_csv('data/qaqc/level_shifts.csv')) 
  }) %>%
    dplyr::bind_rows() %>%
    #intersensor flags require data by SITE, not SITE-PARAMETER
    split(f = .$site, sep = "-") %>%
    purrr::map(., function(data){
      data %>%
        # flag data when water was below freezing
        add_frozen_flag(df = .) %>%
        # flag data when sonde was not submerged in water
        add_unsubmerged_flag() %>%
        # Filter to only un-flagged data, and data that was collected when sonde
        # was actually deployed
        dplyr::filter(is.na(flag) | sonde_employed != 1) %>%
        #... and when the sonde wasn't moved
        dplyr::filter(is.na(depth_change) | depth_change != "sonde moved")
    }) %>% 
    dplyr::bind_rows() %>%
    # reconfigure to site-parameter for future processing
    split(f = list(.$site, .$parameter), sep = "-") %>%
    # remove NULL from the list (indicating a site-param combo that doesn't actually exist)
    purrr::keep(~ !is.null(.))
  
  threshold_lookup <- purrr::map(goodish_data_placeholder, make_threshold_table) %>%
    dplyr::bind_rows()
  
  readr::write_csv(threshold_lookup, 'data/qaqc/seasonal_thresholds.csv')
  
}
```

#### *Test thresholds to flag all data*

*Add flagging functions for each df in all_data_summary_list*

*Pass the dfs in all_data_summary_stats_list through the flagging functions:*

```{r}
single_sensor_flags <- purrr::map(all_data_summary_stats_list, function(data) {
  data %>%
    # flag field visits
    add_field_flag(df = .) %>%
    # Replace any NTU > 1000 with 1000,
    # add "raw" data column ahead of modifying any `mean` values:
    fix_turbidity(df = .) %>%
    # incorporate post-corrected values for development of thresholds:
    # This function is still very much a WIP!
    fix_calibration(df = ., cal_errors = readxl::read_excel("data/calibration_error_log.xlsx")) %>%
    # flag instances outside the spec range
    add_spec_flag(df = ., spec_table = yaml::read_yaml("data/qaqc/sensor_spec_thresholds.yml")) %>%
    # flag data outside of seasonal range
    add_seasonal_flag(df = ., threshold_table = threshold_lookup) %>%
    # flag missing data
    add_na_flag(df = .) %>%
    # flag DO noise (STILL WIP)
    find_do_noise(df = .) %>%
    # flag repeating values
    add_repeat_flag(df = .) %>%
    # find times when sonde was moved up/down in housing (placeholder function, needs enhancements!)
    add_depth_shift_flag(df = ., level_shift_table =  readr::read_csv('data/qaqc/level_shifts.csv')) %>%
    # find instances of sensor drift (FDOM, Chl-a, Turbidity only)
    add_drift_flag(df = .) 
})

intersensor_flags <- single_sensor_flags %>%
  dplyr::bind_rows() %>%
  split(f = .$site, sep = "-") %>%
  # flag turbidity sensor drift (STILL WIP)
  #purrr::map(~add_drift_flag(.)) %>%
  # flag times when water was below freezing
  purrr::map(~add_frozen_flag(.)) %>%
  # overflagging correction. remove slope violation flag if it occurs concurrently
  # with temp or depth
  purrr::map(~intersensor_check(.)) %>%
  # flag times when sonde was unsubmerged
  purrr::map(~add_unsubmerged_flag(.)) %>%
  dplyr::bind_rows() %>%
  data.table::data.table() %>%
  # lil' cleanup of flag column contents
  dplyr::mutate(flag = ifelse(flag == "", NA, flag)) %>%
  # transform back to site-parameter dfs
  split(f = list(.$site, .$parameter), sep = "-") %>%
  purrr::discard(~ nrow(.) == 0) %>%
  # Add in KNOWN instances of sensor malfunction
  purrr::map(~add_malfunction_flag(df = ., malfunction_records = sensor_malfunction_notes))
```

```{r}
# Then go across sites to remove
# seasonal threshold flags that occurred up-/down-stream at the same time
# Lastly, if over 50% of data is flagged in a moving 2-hour window, flag ALL 
#the data in that window
final_flags <- intersensor_flags %>%
  # creates new column, "auto_cleaned_flag" that reduces overflagging of drastic system-wide
  # WQ changes
  purrr::map(~network_check(df = ., network = "all")) %>%
  dplyr::bind_rows() %>%
  
  # !!! MESSY ZONE !!!
  # Here, I clean up the flag column to avoid redundant flagging info (e.g., when there's a site visit, and the data spikes. This is explained completely by there being a site visit).
  # if there is no flag, keep it 'flagless'
  dplyr::mutate(auto_cleaned_flag = ifelse(is.na(auto_cleaned_flag), NA,
                                           # if there are auto-generated flags at the same time as a site visit, remove them
                                           ifelse(grepl("site visit|sv window", auto_cleaned_flag), 
                                                  stringr::str_remove_all(auto_cleaned_flag, 
                                                                          "(DO interference|
                                                   repeated value|
                                                   drift|
                                                   missing data|
                                                   outside of seasonal range|
                                                   slope violation|
                                                   outside of sensor specification range|
                                                   outside of sensor realistic range|
                                                   frozen|
                                                   suspect data)"),
                                                  # if the sonde was not deployed, remove all other flags EXCEPT site visits
                                                  ifelse(grepl("sonde not employed", auto_cleaned_flag) & !grepl("site visit|sv window", auto_cleaned_flag), 
                                                         "sonde not employed",
                                                         # if there is a sensor malfunction, remove all other flags EXCEPT site visits
                                                         ifelse(grepl("sensor malfunction", auto_cleaned_flag) & !grepl("site visit|sv window", auto_cleaned_flag), 
                                                                "sensor malfunction",
                                                                # if the sonde was buried, remove all other flags EXCEPT site visits
                                                                ifelse(grepl("sonde burial", auto_cleaned_flag) & !grepl("site visit|sv window", auto_cleaned_flag), 
                                                                       "sonde burial",
                                                                       # if the sonde had known biofouling, remove all other flags EXCEPT site visits
                                                                       ifelse(grepl("sensor biofouling", auto_cleaned_flag) & !grepl("site visit|sv window", auto_cleaned_flag), 
                                                                              "sensor biofouling",
                                                                              auto_cleaned_flag))))))) %>%
  dplyr::mutate(auto_cleaned_flag = map_chr(auto_cleaned_flag, remove_trailing_semicolons)) %>%
  split(f = list(.$site, .$parameter), sep = "-") %>%
  purrr::map(~add_suspect_flag(.)) %>%
  dplyr::bind_rows() %>%
  # Remove lonely "suspect" flags after auto-cleaning of data (i.e., suspect observations
  # that are totally isolated and no longer linked to any "real" quality flag)
  dplyr::mutate(auto_cleaned_flag = ifelse(is.na(auto_cleaned_flag), NA,
                                           ifelse(auto_cleaned_flag == "suspect data" & is.na(lag(auto_cleaned_flag, 1)) & is.na(lead(auto_cleaned_flag, 1)), NA, auto_cleaned_flag))) %>%
  # remove columns we don't need anymore:
  dplyr::select(-c("r2_s_right", "r2_s_center", "r2_l_right", "r2_l_center", "tightest_r", 
                   "failed", "over_50_percent_fail_window_right", "over_50_percent_fail_window_center"))

if (include_verified == TRUE){
  # Save a version of the data set for future verification steps:
  saveRDS(final_flags %>% dplyr::rename(raw_flag = flag,
                                        flag = auto_cleaned_flag) %>% 
            split(f = list(.$site, .$parameter), sep = "-"), 'data/all_data_auto_flagged_post_verification.RDS')
} else {
  
  # Save a version of the data set for future verification steps:
  saveRDS(final_flags %>% dplyr::rename(raw_flag = flag,
                                        flag = auto_cleaned_flag) %>% 
            split(f = list(.$site, .$parameter), sep = "-"), 'data/virridy_verification/all_data_flagged_complete.RDS')
  
}
```
