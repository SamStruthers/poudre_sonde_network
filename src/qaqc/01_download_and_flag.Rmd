---
title: "Organizing raw data"
author: "ROSSyndicate"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  markdown: 
    wrap: 90
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warnings = 'hide', error = FALSE, message = 'hide')
```

Load necessary packages:

```{r}
source("src/package_loader.R")
lapply(c("data.table", "tidyverse", "rvest", "readxl", "lubridate", "zoo", "padr","plotly", "feather", "RcppRoll", "yaml", "ggpubr", "profvis", "janitor"), package_loader)

walk(list.files('src/qaqc/download_and_flag_fxns', pattern = "*.R", full.names = TRUE, recursive = TRUE), source)
```

# Import and collate data

Load field notes and define the start time as the 15 minute preceding the field time
```{r}
raw_field_notes <- read_excel("data/sensor_field_notes.xlsx")
field_notes <- clean_field_notes(raw_field_notes)
```

Merge the data sets from all API pulls:
```{r}
all_data <- munge_api_data(api_path = "data/api/historical_api_data/")
```

### Export collated raw file

```{r}
# This will be a parquet file in the future?
#write_feather(all_data, paste0('data/SOME_FOLDER_FOR_POSTERITY/collated_raw_sonde_v', Sys.Date(), '.feather'))
```

# Level 1 QA-QC

### Temperature at Archery

I think we will want to develop pipelines specific for each site and parameter for processing speed (targets?).
Up until this point all the data from the API is in one huge csv. After this point the data gets split up into site-parameter combinations.
This means that the data that will be incoming will be one large data packet. This should change in the api pull step.

#### Format data
```{r}
# format and summarize data
# Determine each site and parameter in all_data 
sites <- unique(all_data$site)
params <- c("Battery Level", "Baro", "Chl-a Fluorescence", 
            "Depth", "DO", "External Voltage", "ORP", 
            "pH", "Specific Conductivity", "Temperature", "Turbidity")
 
# Constructing a df to iterate over each site-parameter combination
combinations <- crossing(sites, params)

# Make a list of the summarized data
all_data_summary_list <- map2(combinations$sites, 
                         combinations$params, 
                         summarize_site_param,
                         api_data = all_data,
                         field_notes = field_notes) %>% 
  # set the names for the dfs in the list
  set_names(paste0(combinations$sites, "-", combinations$params)) %>% 
  # remove NULL values from the list
  keep(~ !is.null(.))

# Bind rows for each df in list
all_data_summary_df <- bind_rows(all_data_summary_list)
```

#### Append historical flagged data to incoming data
```{r}
# Read in a 6 hour subset of the historical flagged data for each site-parameter combination
# The incoming data will be in 3 hour increments, but the anomaly window flag needs 6 hours of data, so we will need to append to 6 hours of historical data to the incoming data

# Read in the incoming data and the historical flagged data
  # DANGER: this is a cyclical process
incoming_data <- all_data_summary_list
all_data_flagged <- readRDS('data/flagged/all_data_flagged.RDS')

# appended_data_list <- map2() 
```

### Add summary stats and flag new data
The new data that is being flagged 
#### Add summary statistics to the formatted data
```{r}
# will need to make sure that the summary stats are not being calculated for the top of the data that has already been flagged
all_data_summary_stats_list <- map(all_data_summary_list, generate_summary_statistics)
```

## Create a lookup table for site-parameter thresholds to use in flagging strange data. These thresholds are based on data from the 2022 and 2023 field season. In future seasons, this data will be fixed (i.e., unchanging). 

```{r}
threshold_lookup <- map(all_data_summary_stats_list, make_threshold_table) %>%
  bind_rows()

# save the threshold lookup table as a RDS 
saveRDS(threshold_lookup, 'data/summary_stats/threshold_lookup.RDS')

# this does not need to be a yaml solution
# add this to the threshold look up table and then save the threshold look up table 
sensor_spec_ranges <- yaml::read_yaml("src/qaqc/sensor_spec_thresholds.yml")

saveRDS(all_data_summary_stats_list, 'data/summary_stats/all_data_summary_stats_list.RDS')
```

#### Add flags to all dataframes in all_data_summary_stats_list
Add flagging functions for each df in all_data_summary_list

Pass the dfs in all_data_summary_stats_list through the flagging functions:

```{r}
# make sure that data that has already been flagged is not flagged again, except for the large anomaly flag
# ifelse(historical_data == FALSE, {add_flag}, {do nothing})

all_data_flagged <- map(all_data_summary_stats_list, function(data) {
  data %>%
    add_field_flag() %>%
    add_spec_flag() %>%
    add_seasonal_flag() %>%
    add_na_flag() %>%
    add_repeat_flag() %>% 
    kw_add_large_anomaly_flag() %>%
    mutate(mean_public = ifelse(is.na(flag), mean, NA)) %>%
    mutate(historical_data = TRUE)
})

final_flag <- map(all_data_flagged, site_comp_test)

saveRDS(final_flag, 'data/flagged/all_data_flagged.RDS')
write_feather(final_flag %>% bind_rows(), 'data/flagged/all_data_flagged.feather')
```

All data has now gone through all flagging steps and is ready for immediate public viewing.

#### Remove the data from data/api/incoming_api_data and append it to the historical API data
```{r}
# remove the data from the incoming_api_data folder
```
