---
title: "Organizing raw data"
author: "ROSSyndicate"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  markdown: 
    wrap: 90
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, error = FALSE, message = FALSE) 
```

Load necessary packages:

```{r}
source("src/package_loader.R")
lapply(c("data.table", "tidyverse", "rvest", "readxl", "lubridate", "zoo", "padr","plotly", "feather", "RcppRoll", "yaml", "ggpubr"), package_loader)
```

# Import and collate data

Load and munge field calibration files:

```{r}
# cal_files <- list.files("data/calibration_reports", pattern=".html")
# 
# cal_tabler <- function(cal_files){
# 
#   cal <- read_html(file.path("data/calibration_reports/", cal_files)) %>%
#     html_nodes("div") %>%
#     html_text() %>%
#     as_tibble()
# 
#   rdo <- cal %>% filter(grepl("RDO", value)) %>% pull() %>% str_replace_all(., " ", "") %>% tolower()
# 
#   ph_orp <- cal %>% filter(grepl("pH/ORP", value)) %>% pull() %>% str_replace_all(., " ", "") %>% tolower()
# 
#   conductivity <- cal %>% filter(grepl("Conductivity",value)) %>% pull() %>% str_replace_all(., " ", "") %>% tolower()
# 
#   turbidity <- cal %>% filter(grepl("Turbidity",value)) %>% pull() %>% str_replace_all(., " ", "") %>% tolower()
# 
#   # Always the fifth sensor when depth is available:
#   try(depth <- cal %>% .[5,] %>% pull() %>% str_replace_all(., " ", "") %>% tolower())
# 
#   time_mst <- paste0(str_sub(cal_files, -13, -12),":", str_sub(cal_files, -11, -10))
# 
#   date <- paste0(str_sub(cal_files, -22, -19),"-", str_sub(cal_files, -18, -17),"-", str_sub(cal_files, -16, -15))
# 
#   cal_table <- tibble(site = sub("\\_.*", "", cal_files),
# 
#                       DT = ymd_hm(paste(date, time_mst, tz = "MST")),
# 
#                       # Depth
#                       depth_cal_date = "None",
#                       depth_offset = "None",
#                       depth_ref_depth = "None",
#                       depth_ref_offset = "None",
#                       depth_pre_psi = "None",
#                       depth_post_psi = "None",
# 
#                       # Dissolved Oxygen
#                       rdo_slope = str_match(rdo, "slope\\s*(.*?)\\s*offset")[,2],
#                       rdo_offset = str_match(rdo, "offset\\s*(.*?)\\s*mg/l")[,2],
#                       rdo_100 = str_match(rdo, "premeasurement\\s*(.*?)\\s*%satpost")[,2],
#                       rdo_conc = str_match(rdo, "concentration\\s*(.*?)\\s*mg/lpremeasurement")[,2],
#                       rdo_temp = str_match(rdo, "temperature\\s*(.*?)\\s*°c")[,2],
#                       rdo_pressure = str_match(rdo, "pressure\\s*(.*?)\\s*mbar")[,2],
# 
#                       # pH
#                       ph_slope_pre = str_match(ph_orp, "offset1slope\\s*(.*?)\\s*mv/ph")[,2],
#                       ph_offset_pre = str_match(ph_orp, "mv/phoffset\\s*(.*?)\\s*mvslopeandoffset2")[,2],
#                       ph_slope_post = str_match(ph_orp, "offset2slope\\s*(.*?)\\s*mv/ph")[,2],
#                       ph_offset_post = str_match(ph_orp, paste0(ph_slope_post,"mv/phoffset\\s*(.*?)\\s*mvorporp"))[,2],
#                       # Sometimes, the post value can actually be in the high 6 pH... therefore the post measurement regex matching text is conditional
#                       ph_7_nice = str_sub(str_match(ph_orp, "postmeasurementph7\\s*(.*?)\\s*mvcal")[,2], 10, nchar(str_match(ph_orp, "postmeasurementph7\\s*(.*?)\\s*mvcal")[,2])),
#                       ph_7_other = str_sub(str_match(ph_orp, "postmeasurementph6\\s*(.*?)\\s*mvcal")[,2], 10, nchar(str_match(ph_orp, "postmeasurementph6\\s*(.*?)\\s*mvcal")[,2])),
#                       ph_7 = ifelse(is.na(ph_7_nice), ph_7_other, ph_7_nice),
# 
#                       # ORP
#                       #Newly encountered thing: sometimes the calibration report calls the ORP standard Zobell's, sometimes it's just called "ORP Standard":
#                       orp_offset = ifelse(is.na(str_match(ph_orp, "zobell'soffset\\s*(.*?)\\s*mvtemperature")[,2]),
#                                           str_match(ph_orp, "orpstandardoffset\\s*(.*?)\\s*mvtemperature")[,2],
#                                           str_match(ph_orp, "zobell'soffset\\s*(.*?)\\s*mvtemperature")[,2]),
# 
#                       # Conductivity
#                       tds_conversion_ppm = str_sub(str_match(conductivity, "tdsconversionfactor\\s*(.*?)\\s*cellconstant")[,2], 6, nchar(str_match(conductivity, "tdsconversionfactor\\s*(.*?)\\s*cellconstant")[,2])),
#                       cond_cell_constant = str_match(conductivity, "cellconstant\\s*(.*?)\\s*referencetemperature")[,2],
#                       cond_pre = str_match(conductivity,paste0(str_match(conductivity,
#                                                                          "premeasurementactual\\s*(.*?)\\s*specificconductivity")[,2],"specificconductivity\\s*(.*?)\\s*µs/cmpost"))[,2],
#                       cond_post = str_match(conductivity,paste0(str_match(conductivity,
#                                                                           "postmeasurementactual\\s*(.*?)\\s*specificconductivity")[,2],"specificconductivity\\s*(.*?)\\s*µs/cm"))[,2],
# 
#                       # Turbidity
#                       ntu_slope = "None",
#                       ntu_offset = "None",
#                       ntu_10 = "None",
#                       ntu_100 = "None") %>%
# 
#     select(-c(ph_7_nice, ph_7_other))
# 
#   # Not all sondes have depth. So, we "try" to get the values.
#   try(cal_table <- cal_table %>%
#         mutate(
#           # Depth
#           depth_cal_date = str_match(depth, "lastcalibrated\\s*(.*?)\\s*calibrationdetails")[,2],
#           depth_offset = str_match(depth, "zerooffset\\s*(.*?)\\s*psireferencedepth")[,2],
#           depth_ref_depth = str_match(depth, "psireferencedepth\\s*(.*?)\\s*ftreferenceoffset")[,2],
#           depth_ref_offset = str_match(depth, "ftreferenceoffset\\s*(.*?)\\s*psipremeasurement")[,2],
#           depth_pre_psi = str_match(depth, "psipremeasurement\\s*(.*?)\\s*psipostmeasurement")[,2],
#           depth_post_psi = str_match(depth, "psipostmeasurement\\s*(.*?)\\s*psi")[,2]))
# 
# 
# 
#   # Not all sondes have turbidity. So, we "try" to get the values.
#   try(cal_table <- cal_table %>%
#         mutate(
#           # Turbidity
#           ntu_slope = str_match(turbidity, "slope\\s*(.*?)\\s*offset")[,2],
#           ntu_offset = str_match(turbidity, "offset\\s*(.*?)\\s*ntu")[,2],
#           ntu_10 = str_match(turbidity, "calibrationpoint1premeasurement\\s*(.*?)\\s*ntupost")[,2],
#           ntu_100 = str_match(turbidity, "calibrationpoint2premeasurement\\s*(.*?)\\s*ntupost")[,2]))
# 
#   cal_table <- cal_table %>%
#     mutate(
#       #Factory Defaults
#       factory_defaults = paste0(ifelse(is.na(ntu_slope), "Turbidity ", ""),
#                                 ifelse(is.na(rdo_slope), "RDO ", ""),
#                                 ifelse(is.na(ph_slope_post), "pH ", ""),
#                                 ifelse(is.na(orp_offset), "ORP ", ""),
#                                 ifelse(is.na(cond_post), "Conductivity ", ""),
#                                 ifelse(is.na(depth_cal_date), "Depth ", "")))
#   cal_table
# 
# }
# 
# cal_table <- map_dfr(cal_files, cal_tabler) %>%
#   distinct(.keep_all = TRUE) %>%
#   group_by(site) %>%
#   mutate(DT = round_date(DT, "15 minutes")) %>%
#   # filter for years that are 2022 and greater
#   filter(year(DT) >= 2022)
# # this creates a lot of errors, is it okay to ignore them all? Just make a note that these will create a lot of errors.
# 
# rm(cal_files, cal_tabler, package_loader)
```

Load field notes and define the start time as the 15 minute preceding the field time

```{r}
# Pulling in field notes and adding relevant datetime columns
field_notes <- read_excel("data/sensor_field_notes.xlsx") %>%
  mutate(start_DT = ymd_hm(paste(date, start_time_mst, tzone = "MST"))) %>%
  mutate(#start_DT = with_tz(start_DT, tzone = "MST"),
         DT_round = floor_date(start_DT, "15 minutes"),
         DT_join = as.character(DT_round),
         site = tolower(site),
         season = year(DT_round))

# Determining when the sonde was employed (SE) based on `sensor_pulled` (SP) and `sensor_deployed` (SD) columns.
## when (SP & SD) columns are NOT empty, SE = 0 (employed).
## when (SP) column is NOT empty, but (SD) column is empty, SE = 1 (NOT employed).
## when (SP) column is empty, but (SD) column is NOT empty, SE = 0 (employed).
## when (SP & SD) columns are empty, SE = NA (unknown).

## Downstream, fill() (fill in missing values with previous value)
## is used on the sonde_employed column after it has been joined
## to the pulled API data to determine if the sonde was employed for that data.
deployment_record <- field_notes %>%
  # filter for years that are 2022 and greater 
  filter(year(DT_round) >= 2022) %>% 
  arrange(site, DT_round) %>% 
  group_by(site) %>%
  # `sonde_employed` determines if the sonde is deployed or not. 0 = sonde deployed, 1 = sonde is not deployed
  mutate(sonde_employed = case_when(!is.na(sensor_pulled) & !is.na(sensor_deployed) ~ 0,
                           !is.na(sensor_pulled) & is.na(sensor_deployed) ~ 1,
                           is.na(sensor_pulled) & !is.na(sensor_deployed) ~ 0,
                           is.na(sensor_pulled) & is.na(sensor_deployed) ~ NA),
         last_site_visit = DT_round) %>%
  filter(!is.na(sonde_employed))
```

Munge field notes

```{r}
# field_notes_clean <- field_notes %>% 
#   mutate(sonde_impact = case_when(grepl('sonde', visit_comments, ignore.case = T) ~ 'y',
#                                   grepl('calib', visit_comments, ignore.case = T) ~ 'y',
#                                   sensor_pulled == 'x' ~ 'y',
#                                   sensor_deployed == 'x' ~ 'y',
#                                   grepl('yes', sensors_cleaned, ignore.case = T) ~ 'y'
#   ),
#   end_DT = case_when(sensor_pulled == 'x' ~ ymd_hm(paste0(season, '-12-31 23:59'), tz = 'MST'),
#                      sensor_deployed == 'x' ~ start_DT,
#                      TRUE ~ start_DT + minutes(15)),
#   start_DT = case_when(sensor_deployed == 'x' ~ ymd_hm(paste0(season, '-01-01 00:00'), tz = 'MST'),
#                        TRUE ~ start_DT)) %>% 
#   filter(sonde_impact == 'y', !is.na(start_DT))
```

Merge the data sets from all API pulls:

```{r}
all_data <- list.files(path = "data/api/", full.names = TRUE, pattern = "*.csv") %>%
  map_dfr(~data.table::fread(.) %>% select(-id)) %>%
  # remove overlapping API-pull data
  distinct() %>%
  # remove VuLink data
  filter(!grepl("vulink", name, ignore.case = TRUE)) %>%
  # remove Virridy data (for now)
  filter(!grepl("virridy", name, ignore.case = TRUE)) %>%
  # Convert DT to MST:
  mutate(DT = as_datetime(timestamp, tz = "UTC")) %>%
  mutate(DT = with_tz(DT, tzone = "MST"),
         DT_round = round_date(DT, "15 minutes"),
         DT_join = as.character(DT_round),
         site = tolower(site)) %>%
  # filter for years that are 2022 and greater 
  filter(year(DT_round) >= 2022) %>% 
  # Lastly, we swapped Boxelder's sonde out for Rist's late in 2022:
  mutate(site = ifelse(site == "rist" & DT > "2022-09-20" & DT < "2023-01-01", "boxelder", site))
```

### Export collated raw file

```{r}
# This will be a parquet file in the future?
#write_feather(all_data, paste0('data/SOME_FOLDER_FOR_POSTERITY/collated_raw_sonde_v', Sys.Date(), '.feather'))
```

# Level 1 QA-QC

### Recoding data where sonde was out of water

Filter instances in which a sonde was pulled out of field mid-season (basically a backup filter in case a sonde was pulled out of water but the log didn't get stopped)

```{r}
#this function is not working... not going to debug right now, but this is meant to do the lifting of this code block, but in an automated way... 
# 
# recode_for_maintenance = function(start, end, site) {
#   # start_DT = ymd_hm(start, tz = 'MST')
#   # end_DT = ymd_hm(end, tz = 'MST')
#   all_data %>%
#     setDT(.) %>%
#     mutate(value = if_else(ymd_hms(DT) >= ymd_hms(start) &
#                              ymd_hms(DT) <= ymd_hms(end) &
#                              site == site,
#                            NA_real_,
#                            value),
#            flag = if_else(ymd_hms(DT) >= ymd_hms(start) &
#                             ymd_hms(DT) <= ymd_hms(end) &
#                             site == site,
#                           'recoded for sensor maintenance',
#                           NA_character_))
# }

# preserving this for now.
# all_data <- all_data %>% 
#   setDT(.) %>%
#   # Rist
#   filter(!(ymd_hms(DT_round) >= ymd_hms("2021-09-11 09:00:00") & ymd_hms(DT_round) <= ymd_hms("2022-04-22 14:00:00") & site == "rist"),
#          !(ymd_hms(DT_round) <= ymd_hms("2022-05-30 09:00:00") & site == "rist"),
#          !(ymd_hms(DT_round) >= ymd_hms('2022-05-06 12:00:00') & ymd_hms(DT_round) <= ymd_hms('2022-05-09 14:30:00') & site == "rist"),
#          # Legacy
#          !(ymd_hms(DT_round) >= ymd_hms('2021-12-04 19:30:00') & ymd_hms(DT_round) < ymd_hms('2022-04-06 17:30:00') & site == "legacy"),
#          !(ymd_hms(DT_round) >= ymd_hms('2022-05-24 09:30:00') & ymd_hms(DT_round) < ymd_hms('2022-06-01 13:30:00') & site == "legacy"),
#          !(ymd_hms(DT_round) > ymd_hms('2022-07-08 14:00:00') & ymd_hms(DT_round) <= ymd_hms('2022-07-12 10:00:00') & site == "legacy"),
#          !(ymd_hms(DT_round) >= ymd_hms('2022-08-04 09:50:00') & ymd_hms(DT_round) <= ymd_hms('2022-08-25 16:15:00') & site == "legacy"),
#          !(ymd_hms(DT_round) > ymd_hms('2022-09-07 06:57:00') & ymd_hms(DT_round) <= ymd_hms('2022-09-18 07:00:00') & site == "legacy"),
#          # Timberline
#          !(ymd_hms(DT_round) > ymd_hms('2022-01-01 08:15:00') & ymd_hms(DT_round) <= ymd_hms('2022-04-06 08:15:00') & site == "timberline"),
#          # Archery
#          !(ymd_hms(DT_round) > ymd_hms('2022-10-04 15:00:00') & ymd_hms(DT_round) <= ymd_hms('2022-10-07 16:00:00') & site == "archery"))
```

### Temperature at Archery

I think we will want to develop pipelines specific for each site and parameter for
processing speed (targets?). Here I am starting a pipeline for Archery's temperature:

#### Format data

```{r}
# Determine each site and parameter in all_data 
sites <- unique(all_data$site)
params <- unique(all_data$parameter)
# Constructing a df to iterate over each site-parameter combination
combinations <- crossing(sites, params)

# Function to summarize site-parameter combinations
summarize_site_param <- function(site_arg, parameter_arg, api_data) {
  
  # filter deployment records for the full join
  site_field_notes <- deployment_record %>% 
    filter(site == site_arg)
  
  # filtering the data and generating results
  summary <- tryCatch({ 
    api_data %>%
    filter(site == site_arg & parameter == parameter_arg) %>% 
    select(-name) %>%
    distinct() %>%
    group_by(DT_round) %>% # site & parameter does not need to be here anymore
    # to do: preserve values used with nest()
    summarize(mean = as.numeric(mean(value, na.rm = T)),
              diff = abs(min(value, na.rm = T) - max(value, na.rm = T)),
              n_obs = n()) %>%
    ungroup() %>%
    arrange(DT_round) %>%
    # pad the dataset so that all 15-min timestamps are present
    pad(by = "DT_round", interval = "15 min") %>%
    mutate(DT_join = as.character(DT_round),
           site = site_arg,
           parameter = parameter_arg,
           flag = NA) %>% 
    full_join(filter(dplyr::select(site_field_notes, sonde_employed, last_site_visit, DT_join, site)),
                 by = c('DT_join', 'site')) %>% 
    # Use fill() to determine when sonde was in the field, and when the last site visit was. 
    fill(c(sonde_employed, last_site_visit))
    },
    
    error = function(err) {
      # error message
      cat("An error occurred with site ", site_arg, " parameter ", parameter_arg, ".\n")
      cat("Error message:", conditionMessage(err), "\n")
      flush.console() # Immediately print the error messages
      NULL  # Return NULL in case of an error
    })
    
    return(summary)
}

# Make a list of the summarized data
all_data_summary_list <- map2(combinations$sites, 
                         combinations$params, 
                         summarize_site_param,
                         api_data = all_data) %>% 
  # set the names for the dfs in the list
  set_names(paste0(combinations$sites, "-", combinations$params)) %>% 
  # remove NULL values from the list
  keep(~ !is.null(.))

# Bind rows for each df in list
all_data_summary_df <- bind_rows(all_data_summary_list)
```
#### Add summary statistics to the formatted data

```{r}
generate_summary_statistics <- function(site_param_df) {
  # This should get wrapped up in a function to map over all_data_summary_list
  summary_stats_df <- site_param_df %>%
    # ... so that we can get the proper leading/lagging values across our entire timeseries:
    mutate(
      # Add the next value and previous value for mean.
      front1 = lead(mean, n = 1),
      back1 = lag(mean, n = 1),
      # Add the median for a point centered in a rolling median of 7 points.
      rollmed = roll_median(mean, n = 7, align = 'center', na.rm = F, fill = NA_real_),
      # Add the mean for a point centered in a rolling mean of 7 points.
      rollavg = roll_mean(mean, n = 7, align = 'center', na.rm = F, fill = NA_real_),
      # Add the standard deviation for a point centered in a rolling mean of 7 points.
      rollsd = roll_sd(mean, n = 7, align = 'center', na.rm = F, fill = NA_real_),
      # Determine the slope of a point in relation to the point ahead and behind.
      slope_ahead = abs(front1 - mean)/15,
      slope_behind = abs(mean - back1)/15,
      # add some summary info for future us
      month = month(DT_round),
      year = year(DT_round),
      y_m = paste(year, '-', month)
      ) %>% 
    group_by(y_m) %>% 
    # KW:: wondering if, there's a way to develop the sd using ONLY the range of data in
    # the 25%-75% percentile range. Therefore, we don't include any of the weird outliers
    # when trying to flag this way...
    mutate(ym_sd = sd(mean, na.rm = T)) %>% 
    ungroup() # does this do anything for downstream analysis?
  
  return(summary_stats_df)
}
all_data_summary_stats_list <- map(all_data_summary_list, generate_summary_statistics)
```

#### Add the flags
Add flagging functions for each df in all_data_summary_list

```{r}
# flag addition function 
# This function will be called inside of add_x_flags() functions (ex. add_field_flags())
# This function will be used to simply add flags into the flag column
add_flag <- function(df, condition_arg, description_arg) {
  df <- df %>% mutate(flag = case_when(
    {{condition_arg}} ~ if_else(is.na(flag), paste(description_arg), paste(flag, description_arg, sep = ";\n")),
    TRUE ~ flag)) # <- KW question: what is this part of your code doing? 
  return(df)
}

# Flag alter function
# This function can be used to alter flags that have been established
alter_flag <- function(df, condition_arg, old_description_arg, new_description_arg) {
  df <- df %>% mutate(flag = case_when(
    {{condition_arg}} & str_detect(flag, old_description_arg) ~ str_replace(flag, old_description_arg, new_description_arg),
    TRUE ~ flag))
  return(df)
}

# Example with field flags

# creating add_field_flags function
add_field_flags <- function(df) {
  df <- df %>% 
    # To use add_flag in a pipeline just input a condition for a flag and your description for the flag.
    add_flag(sonde_employed == 1, "sonde not employed") %>% 
    add_flag(as.character(last_site_visit) == as.character(DT_round), "site visit")
  return(df)
}
```

```{r}
# Add the function to add the range flags using the parameter thresholds yaml file

parameter_ranges <- yaml::read_yaml("src/qaqc/parameter_thresholds.yml")

# sensor spec ranges flagging function
  # Some parameters are derived from a combination of other parameters. If any of those are wrong then they should be flagged. 
add_range_flags <- function(df) {
  
  # get the parameter from the parameter column in the df of interest
  parameter_name <- unique(na.omit(df$parameter))
  # Pull the sensor specification range from the yaml file
  sensor_min <- parameter_ranges[[parameter_name]]$sensor_specifications$min
  sensor_max <- parameter_ranges[[parameter_name]]$sensor_specifications$max
  # Pull the lab bound range from the yaml file
  lab_min <- parameter_ranges[[parameter_name]]$lab_bounds$min
  lab_max <- parameter_ranges[[parameter_name]]$lab_bounds$max

  df <- df %>%
    # adding sensor range flags
    add_flag(parameter == parameter_name & (mean < sensor_min | mean > sensor_max),
             paste0("out of (", parameter_name,") sensor specification range")) %>%
    # adding lab bound flags
    add_flag(parameter == parameter_name & (mean < lab_min | mean > lab_max),
             paste0("out of (", parameter_name,") lab bounds"))
  
  return(df)
  
}
```

For wonky data next to each other (defined by roll sd) - here we make some monthly summaries to help with this:

```{r}
# flag data points that are outside the range of 3 sds from the mean of previous + future 3 data points:
# NOW, we are using the monthly sd instead of the rolling sd...

  # keep this df name temperature_archery_bound_flags bc the bounding flagging
  # function was the last one that was applied

# This has been moved down to the all_data_flagged step ----

# temperature_archery_bound_flags <- temperature_archery_bound_flags %>% 
#   add_flag((mean <= rollavg - (3 * ym_sd) | mean >= rollavg + (3 * ym_sd)), "Outside SD range")
```

Pass the dfs in all_data_summary_stats_list through the flagging functions
```{r}
# Set the slope threshold for the functions below
# This will change whent the slopes are incorporated into the yaml file,
# for now this is just for temperature data
slope_thresh = 1/15


# This adds the flags to all the dfs inside of all_data_summary_stats_list
all_data_flagged <- map(all_data_summary_stats_list, ~ .x %>% 
  add_field_flags %>% 
  add_range_flags %>% 
  # SD flag
  add_flag((mean <= rollavg - (3 * ym_sd) | mean >= rollavg + (3 * ym_sd)), "Outside SD range") %>% 
  # For data that repeats
  add_flag((mean == front1 | mean == back1), "Repeated value") %>%
  # Slope flag
  add_flag((slope_ahead >= slope_thresh | slope_behind >= slope_thresh), "slope flag suspect") %>% 
  alter_flag((slope_ahead >= slope_thresh & slope_behind >= slope_thresh), "slope flag suspect", "slope flag actual") %>% 
  # Missing data flag
  add_flag(is.na(mean), "missing data"))

# add pass/fail column and apply this to every df 
add_verification_column <- function(df) {
  df <- df %>% 
    mutate(verification = case_when(
      str_detect(flag, "missing data") ~ "fail",
      is.na(flag) ~ "pass",
      TRUE ~ NA
    ))
  return(df)
}

all_data_flagged <- map(all_data_flagged, add_verification_column)
```

#### Visualize the flags

```{r}
# Current vis:
ggplot() +
  geom_point(data = filter(all_data_flagged[["archery-Temperature"]], is.na(flag)), 
             aes(x=DT_round, y = mean)) +
  geom_point(data = filter(all_data_flagged[["archery-Temperature"]], !is.na(flag)), 
             aes(x=DT_round, y = mean, color = flag)) +
  theme_bw() +
  theme(legend.position = 'bottom') 
```

Visualizing daily plots
```{r}
# Visualizing daily plots

## This function will generate a list of plots for site-parameters
## that have been tagged by a specific flag 
generate_daily_flag_plots <- function(site_arg, parameter_arg, flag_arg = NULL) {
  
  # Generating df name to pull from all_data_flagged list
  site_param <- paste0(site_arg, "-", parameter_arg)
  # filter for all the days that are tagged within the site-param df of interest
  site_flag_dates <- all_data_flagged[[site_param]] %>%
      filter(if (is.null(flag_arg)) !is.na(flag) else str_detect(flag, flag_arg)) %>%
      group_by(day(DT_join)) %>%
      slice(1)
  
  # for loop to generate plots for everyday that was tagged by a flag
  plot_list <- list()
  
  for (i in 1:nrow(site_flag_dates)) {
    
    flag_title <- site_flag_dates$flag[i]
    flag_year <- site_flag_dates$year[i]
    flag_month <- site_flag_dates$month[i]
    flag_day <- site_flag_dates$DT_round[i]

    plot_data <- all_data_flagged[[site_param]] %>%
      filter(year == flag_year,
             month == flag_month,
             day(DT_round) == day(flag_day))

    plot <- ggplot(data = plot_data) +
      geom_point(aes(x=DT_round, y = mean, color = flag)) +
      # exceeding sd visualized
      geom_line(aes(x = DT_round, y = rollavg, color = "mean"), show.legend = TRUE) +
      geom_ribbon(aes(ymin = rollavg - ym_sd, ymax = rollavg + ym_sd, x = DT_round), alpha = 0.1, color = NA) +
      geom_ribbon(aes(ymin = rollavg - (ym_sd*2), ymax = rollavg + (ym_sd*2), x = DT_round), alpha = 0.1, color = NA) +
      geom_ribbon(aes(ymin = rollavg - (ym_sd*3), ymax = rollavg + (ym_sd*3), x = DT_round), alpha = 0.1, color = NA) +
      # exceeding slope visualized
      geom_vline(data = (plot_data %>% filter(is.na(mean))), aes(xintercept = DT_round, color = flag)) +
      theme_bw() +
      theme(legend.position = 'bottom') +
      ggtitle(paste(flag_title ,"at", site_arg, "on", as.character(flag_day))) + # HERE
      labs(x = "Datetime",
           y = "Mean")

    plot_list[[as.character(flag_day)]] <- plot

  }
  
  return(plot_list)
  
}

generate_daily_flag_plots("archery", "Temperature", "Repeated value")
```

Visualizing weekly plots
```{r}
# Visualizing weekly data ----

generate_weekly_flag_plots <- function(site_arg, parameter_arg, flag_arg = NULL) { 
  # vector of sites in the order that they are in spatially
  # some sites have some funkiness going on
  sites_order <- c("tamasag", # rist 
                "legacy",
                "lincoln",
                "timberline",
                "prospect",
                "boxelder", # elc
                "archery",
                "river bluffs")
  # determining the index for the site of interest.
  site_index <- which(sites_order == site_arg)
  
  # Generating df name to pull from all_data_flagged list
  site_param <- paste0(site_arg, "-", parameter_arg)
  # filter for all the days that are tagged within the site-param df of interest
  site_flag_dates <- all_data_flagged[[site_param]] %>% 
      filter(if (is.null(flag_arg)) !is.na(flag) else str_detect(flag, flag_arg)) %>%
      group_by(day(DT_join)) %>%
      slice(1)
  
  # This for loop generates an overlayed plot of weekly data for the site of 
  # interest sandwiched by the site above and below it for each day that was 
  # tagged by a flag of interest
  plot_list <- list()

  for(i in 1:nrow(site_flag_dates)) {
    
    flag_title <- site_flag_dates$flag[i]
    flag_year <- site_flag_dates$year[i]
    flag_month <- site_flag_dates$month[i]
    flag_day <- site_flag_dates$DT_round[i]
    # Getting the prior and subsequent 3 days to the flag day
    start_day <- flag_day - days(3)
    end_day <- flag_day + days(3)
    
    # filtering dfs of interest for the weeks where a flag was detected
    site_df <- all_data_flagged[[site_param]] %>%
      filter(year == flag_year,
      month == flag_month,
      DT_round >= start_day & DT_round <= end_day)
    
    # TryCatch used here to avoid erroring out on the first and last values of
    # sites_order object (there is no prior/next record after the first/last record).
    # Return df as NULL in case of an error
    prev_site_df <- NULL
    next_site_df <- NULL
    
    tryCatch({
      previous_site <- paste0(sites_order[site_index-1],"-",parameter_arg)
      prev_site_df <- all_data_flagged[[previous_site]] %>%
        filter(year == flag_year,
        month == flag_month,
        DT_round >= start_day & DT_round <= end_day)},
      error = function(err) {
        cat("No previous site")}) 

    tryCatch({
      next_site <- paste0(sites_order[site_index+1],"-",parameter_arg)
      next_site_df <- all_data_flagged[[next_site]] %>%
        filter(year == flag_year,
        month == flag_month,
        DT_round >= start_day & DT_round <= end_day)},
      error = function(err) {
        cat("No next site")})

    # Bind all three dfs
    week_plot_data <- list(site_df, prev_site_df, next_site_df) %>% 
      # remove NULL values from the list
      keep(~ !is.null(.)) %>% 
      bind_rows()
    
    # Using the data from the day where a flag was detected to generate a window
    # to easily distinguish the data of interest in comparison with the rest of 
    # the data
    site_day_data <- all_data_flagged[[site_param]] %>%
      filter(year == flag_year,
           month == flag_month,
           day(DT_round) == day(flag_day))

    week_plot <- ggplot(data = week_plot_data, aes(x=DT_round, y=mean, color=site)) +
      geom_rect(data = site_day_data, aes(xmin = min(DT_round), xmax = max(DT_round),
                                          ymin = -Inf, ymax = Inf),
                fill = "grey",
                alpha = 0.01,
                color = NA)+
      geom_line() +
      theme_bw() +
      theme(legend.position = 'bottom') +
      ggtitle(paste(flag_title,"at", site_arg, "on", as.character(flag_day))) +
      labs(x = "Datetime",
           y = "Mean")

    plot_list[[paste(site_arg, parameter_arg, as.character(flag_day))]] <- week_plot
  }
  
  return(plot_list)
}

generate_weekly_flag_plots("archery", "Temperature", "slope flag suspect")
```

Stack the weekly and daily plots
```{r}
# function to combine weekly and daily plots 
stack_plots <- function(week_plot, day_plot) {
  ggarrange(week_plot, day_plot, nrow = 2, ncol = 1)
}

# This function lets you combine the weekly and daily plots into one stacked plot
# to compare the two. 
weekly_daily_flag_plots <- function(site_arg, parameter_arg, flag_arg) {
  
  # Call on the weekly and daily functions and fill their args with this 
  # functions args
  weekly_plot_list <- generate_weekly_flag_plots(site_arg = site_arg, parameter_arg = parameter_arg, flag_arg = flag_arg)
  daily_plot_list <- generate_daily_flag_plots(site_arg = site_arg, parameter_arg = parameter_arg, flag_arg = flag_arg)
  
  # These two functions should alway returnt the same amount of plots, so we can
  # use map2() to stack them with combine_plots()
  weekly_daily_plots <- map2(weekly_plot_list, daily_plot_list, stack_plots)
  return(weekly_daily_plots)
}

weekly_daily_flag_plots("archery", "Temperature", "slope flag suspect")
```

Histograms of all parameter/site combos
```{r}
# generate basic histograms for dfs
generate_histograms <- function(df, df_index) {
  
  site_param <- toupper(sub("-", " ", df_index, fixed = TRUE))
  title <- paste("Histogram of ", site_param)
  
  histogram <- ggplot(data = df, aes(x = mean)) +
    geom_histogram() +
    labs(title = title)

  return(histogram)
}

# use imap to use the index in the list for the plot titles
result <- purrr::imap(all_data_flagged, ~generate_histograms(.x, .y))
result[[1]]
```


