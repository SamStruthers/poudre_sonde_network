---
title: "HydroVu API Pull"
author: "B Steele, modified by Sam Struthers"
date: "`r Sys.Date()`"
output: html_document
---

# Purpose

This R-markdown accesses the HydroVu server via the API to download data within this scripted workflow. See the README.md file for directions to set up the credentials.yml file sourced in this script.

## Requirements

This markdown requires the package 'HydroVuR', which is currently forked in B's GH.

## Workspace Set Up

For grabbing data from HydroVu, we'll be using the credentials.yml file and the package 'HydroVuR', which is available on GitHub through devtools.

```{r}
devtools::install_github("steeleb/HydroVuR")
library('tidyverse')
library('HydroVuR')
source('src/api_pull/hv_getdata_id.R')
source('src/api_pull/hv_locations_all.R')
```

## Accessing the API

First thing, we're going to read in our client id and client secret for accessing the HydroVu API. Don't print these out in this chunk: if you save your Rmd file with the chunk history, that's another way that the world can see your data. ðŸ™ƒ

```{r}
creds = yaml::read_yaml('src/api_pull/credentials.yml')

client_id = as.character(creds['client'])
client_secret = as.character(creds['secret'])

# get a token for location lists and data access
token <- hv_auth(client_id, client_secret)
```

Download the raw data on HydroVu based on time frame and location. Start/end times must EXACTLY match this format:

YYYY-MM-DD HH:MM:SS MDT

```{r}
# where the api data gets saved:
dump_dir <- 'data/api/'

# Code for making the start time the day of the last saved API pull:
# start <- sort(list.files(dump_dir) %>%
#                  str_sub(start = -14, end = -5), decreasing = TRUE)[1]
#... but, just the last 7 days for now
start <- Sys.time() - lubridate::days(7)
end <- Sys.time()
sites <- c("Tamasag", "Legacy", "Timberline", "Prospect", "Boxelder", "Archery", "River Bluffs")

api_puller <- function(site, 
                       dump_dir = 'data/api/', 
                       start = Sys.time() - lubridate::days(7),
                       end = Sys.time()){
  
  locs <- hv_locations_all(token)
  
  # make a list of site names
  
  options(scipen = 999)
  
  site_loc <- locs %>%
    filter(grepl(site, name))
  
  site_loc_list <- site_loc$id
  
  # Get data for each site location. Note this maps over the entire list of locations, 
  # many of which are unlikely to be active during the time you specify. Don't freak out if
  # you see a bunch of '404 Not Found' errors, you're just seeing the list of locations 
  # that are not active. The data frame 'alldata' should contain your data from all applicable
  # sites during the time frame indicated. Note that this will take some time (one month of
  # data for 5 sites takes ~10 mins. Be patient!
  
  # Add date range you are interested in; data are stored in HydroVu in UTC
  utc_start_date <- format(as.POSIXct(format(start, tz = "UTC"), tz = "UTC"), format = "%Y-%m-%d %H:%M:%S")
  
  utc_end_date <- format(as.POSIXct(format(end, tz = "UTC"), tz = "UTC"), format = "%Y-%m-%d %H:%M:%S")
  timezone = 'UTC'
  
  # Map over the location ids 
  alldata <- map(site_loc_list, 
                 hv_data_id, 
                 start_time = utc_start_date, 
                 end_time = utc_end_date, 
                 token = token, 
                 tz = timezone)
  
  # grab only locations with data (stored as a data frame) / drop 404 errors
  filtered <- purrr::keep(alldata, is.data.frame)
  
  # bind lists together (now that all are dataframes, we can just collate quickly)
  one_df <- bind_rows(filtered) %>% 
    rename(id = Location,
           parameter = Parameter,
           units = Units) %>% 
    left_join(., locs) %>% 
    mutate(site = site) %>%
    select(site, id, name, timestamp, parameter, value, units)
  
  ## Save your data
  
  write_csv(one_df,
            paste0(dump_dir, "/", site, "_", Sys.Date(), ".csv"))
  
  return(one_df)
  
}

data <- sites %>% map(~api_puller(site = .)) %>%
  bind_rows()
```

