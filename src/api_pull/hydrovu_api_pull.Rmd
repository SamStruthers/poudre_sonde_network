---
title: "HydroVu API Pull"
author: "ROSSyndicate"
date: "`r Sys.Date()`"
output: html_document
---

```{r}
knitr::opts_chunk$set(echo = TRUE, evaluate = FALSE)
```

# Purpose

This R-markdown accesses the HydroVu server via the API to download data within this scripted workflow. See the README.md file for directions to set up the credentials.yml file sourced in this script.

## Requirements

This markdown requires the package "HydroVuR", which is currently forked in B's GH.

## Workspace Set Up

For grabbing data from HydroVu, we'll be using the credentials.yml file and the package 'HydroVuR', which is available on GitHub through devtools.

```{r}
devtools::install_github("steeleb/HydroVuR")

source("src/package_loader.R")
lapply(c("tidyverse", "lubridate", "HydroVuR"), package_loader)

source("src/api_pull/hv_getdata_id.R")
source("src/api_pull/hv_locations_all.R")
```

## Accessing the API

First thing, we're going to read in our client id and client secret for accessing the HydroVu API. Don't print these out in this chunk: if you save your Rmd file with the chunk history, that's another way that the world can see your data. ðŸ™ƒ

```{r}
creds = yaml::read_yaml("src/api_pull/credentials.yml")

client_id = as.character(creds["client"])
client_secret = as.character(creds["secret"])

# get a token for location lists and data access
hv_token <- hv_auth(client_id, client_secret)
```

Download the raw data on HydroVu based on time frame and location. Start/end times must EXACTLY match this format:

YYYY-MM-DD HH:MM:SS MDT

```{r}
# where the api data gets saved:
dump_dir <- "data/api/"
# into incoming folder
# after it has been used this data will go into the outgoing folder
# after it has been used from the outgoing folder it will be appended to the historical data

# Code for making the start time the day of the last saved API pull:
start <- sort(list.files(dump_dir, pattern = "*.csv") %>%
                str_sub(start = -14, end = -5), decreasing = TRUE) %>%
  .[1]
end <- Sys.time()

#... or, manual selection of start/end time
 start <- as.POSIXct("2018-01-01 01:00:00 MDT")
 end <- as.POSIXct("2023-07-30 01:00:00 MDT")

sites <- c("Rist", 
           "Tamasag", # Rist was moved to Tamasag beginning of 2023
           "Legacy",
           "Lincoln",
           "Timberline",
           "Prospect", # no data in 2020
           "ELC",
           "Boxelder", # ELC was moved to Boxelder late summer 2022
           "Archery",
           "River Bluffs"
           )

api_puller <- function(site, 
                       dump_dir = 'data/api/incoming_api_data/', 
                       start,
                       end){
  
  locs <- hv_locations_all(token)
  
  # make a list of site names
  
  options(scipen = 999)
  
  site_loc <- locs %>%
    filter(grepl(site, name))
  
  site_loc_list <- site_loc$id
  
  # Get data for each site location. Note this maps over the entire list of locations, 
  # many of which are unlikely to be active during the time you specify. Don't freak out if
  # you see a bunch of '404 Not Found' errors, you're just seeing the list of locations 
  # that are not active. The data frame 'alldata' should contain your data from all applicable
  # sites during the time frame indicated. Note that this will take some time (one month of
  # data for 5 sites takes ~10 mins. Be patient!
  
  # Add date range you are interested in; data are stored in HydroVu in UTC
  # Here, a way to find the most recent download of the data. Use this as the start date to 
  # reduce overlapping data
  
  utc_start_date <- format(as.POSIXct(format(start, tz = "UTC"), tz = "UTC"), format = "%Y-%m-%d %H:%M:%S")
  
  utc_end_date <- format(as.POSIXct(format(end, tz = "UTC"), tz = "UTC"), format = "%Y-%m-%d %H:%M:%S")
  
  timezone = "UTC"
  
  # Map over the location ids 
  alldata <- map(site_loc_list, 
                 hv_data_id, 
                 start_time = utc_start_date, 
                 end_time = utc_end_date, 
                 token = token, 
                 tz = timezone)
  
  # grab only locations with data (stored as a data frame) / drop 404 errors
  filtered <- purrr::keep(alldata, is.data.frame)
  
  if(length(filtered) == 0){
    
    print(paste0("No data at ", site, " during this time frame"))
    
  } else {
    
    # bind lists together (now that all are dataframes, we can just collate quickly)
    one_df <- bind_rows(filtered) %>% 
      rename(id = Location,
             parameter = Parameter,
             units = Units) %>% 
      left_join(., locs) %>% 
      mutate(site = tolower(site)) %>%
      select(site, id, name, timestamp, parameter, value, units)
    
    ## Save your data
    
    write_csv(one_df,
              paste0(dump_dir, "/", site, "_", lubridate::as_date(end), ".csv"))
    
    return(one_df)
  }
  
}

sites %>% walk(~api_puller(site = .,
                           start = start,
                           end = end))
```

```{r}
# append to historical data function
# this will take the data from the incoming_data folder and append it to the historical_data folder

```

```{r}
# the site arg will remain the same

# we will have to change the start and end dates for each pull depending on the last date that is in the all_data_flagged RDS file

all_data_flagged_bound <- read_feather('data/flagged/all_data_flagged.feather')
all_data_flagged_dfs <- readRDS('data/flagged/all_data_flagged.RDS')

# create a function to extract the last value of DT_round
get_last_DT_round <- function(df) {
  tail(df$DT_round, n = 1)
}

# apply the function to each data frame in all_data_flagged_dfs
last_DT_round <- lapply(all_data_flagged_dfs, get_last_DT_round)

# convert the resulting list to a data frame
last_DT_round_df <- tibble(site = names(all_data_flagged_dfs),
                               last_DT_round = unlist(last_DT_round)) %>%
                               # pull the site name from the index
                               mutate(site = sub("-.*", "", site),
                               # convert the Unix timestamp to a POSIXct object
                               last_DT_round = as.POSIXct(.$last_DT_round, origin = "1970-01-01")) %>%
                               # summarize and find the earliest date for each site
                              group_by(site) %>%
                              summarize(last_DT_round = min(last_DT_round))

# view the resulting data frame
last_DT_round_df %>% View()

# last_DT_round_df will take the place of the start and end dates in the api_puller function


# sites %>% walk(~api_puller(site = .,
#                            start = start,
#                            end = end))
```

```{r, eval = FALSE}
walk_df <- last_DT_round_df

# this is what we will do to update all of the sites at once, before we start appending every 3 hours
# walk_df %>%
  walk2(~api_puller(site = .$site,
    start = .$last_DT_round,
    end = Sys.time()))

# then it will go into the flagging pipeline
  # the flagged data is the final product right now

# after the big one is done, this is how we will update it every 3 hours:
## GitHub Actions will run this script every 3 hours, so we will only need to update the start and end dates
all_data_flagged_dfs
```