---
title: "2025 Hydro Vu API Pull"
author: "Sam Struthers - CSU ROSSyndicate"
date: "`r Sys.Date()`"
output: html_document
---

# Set up

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# loading packages
package_loader <- function(x) {
  if (x %in% installed.packages()) {
    suppressMessages({
      library(x, character.only = TRUE)
    })
  } else {
    suppressMessages({
      install.packages(x)
      library(x, character.only = TRUE)
    })
  }
}

invisible(
  lapply(c("arrow",
           "data.table",
           "httr2",
           "tidyverse",
           "lubridate",
           "zoo",
           "padr",
           "stats",
           "RcppRoll",
           "yaml",
           "here",
           "fcw.qaqc",
           "furrr"
  ),
  package_loader)
)

# Set up parallel processing
num_workers <- min(availableCores() - 1, 4) # Use at most 4 workers
plan(multisession, workers = num_workers)
set_furr_options <- furrr_options(
  globals = TRUE,
  packages = c("arrow", "data.table", "httr2", "tidyverse","dplyr", "lubridate", "zoo",
               "padr", "stats", "RcppRoll", "yaml", "here", "fcw.qaqc")
)

# suppress scientific notation to ensure consistent formatting
options(scipen = 999)

```

## Setting up the site/time range for the data pull

```{r}
#Adjust dates for specific pull
mst_start <- ymd_hms("2025-03-01 00:00:00", tz = "America/Denver")
mst_end <- ymd_hms("2025-11-30 23:59:59", tz = "America/Denver")

sites <- c(
  #Upper sites
  "joei", "cbri", "chd", "pbr", "pfal", "pman", "pbd",
  #Lower sites
  "bellvue","salyer", "udall", "riverbend","cottonwood", "elc","archery","riverbluffs", 
  #lower tribs
  "boxcreek", "springcreek")

```

## Establishing directory paths.

```{r}
#folder with downloaded logs
log_directory <- here("data", "sensor_data", "2025")  #Change as needed

#Change as needed
staging_directory <- here("data","manual_data_verification","2025_cycle","hydro_vu_pull" ,"raw_data")
flagged_temp_dir <- here("data","manual_data_verification","2025_cycle","hydro_vu_pull" ,"flagged_data_temp")
flagged_directory <- here("data","manual_data_verification","2025_cycle","hydro_vu_pull" ,"flagged_data")

#check to make sure these all exist and create if needed
if (!dir.exists(staging_directory)) {
  dir.create(staging_directory, recursive = TRUE)
}
if (!dir.exists(flagged_temp_dir)) {
  dir.create(flagged_temp_dir, recursive = TRUE)
}
if (!dir.exists(flagged_directory)) {
  dir.create(flagged_directory, recursive = TRUE)
}

```

## Read in thresholds and credentials

```{r}
# Read in the threshold data first
sensor_thresholds <- read_yaml(here("data","field_notes","qaqc",  "sensor_spec_thresholds.yml"))
season_thresholds <- read_csv(here("data","field_notes","qaqc", "updated_seasonal_thresholds_2025_sjs.csv"), show_col_types = FALSE) %>%
  fix_site_names()

# Read in credentials
mWater_creds <- read_yaml(here("creds", "mWaterCreds.yml"))

hv_creds <- read_yaml(here("creds", "HydroVuCreds.yml"))
hv_token <- hv_auth(client_id = as.character(hv_creds["client"]),
                    client_secret = as.character(hv_creds["secret"]))

```

# Data Pulls

## Pulling in field notes from mWater
```{r}

mWater_data <- load_mWater(creds = mWater_creds)

all_field_notes <- grab_mWater_sensor_notes(mWater_api_data = mWater_data)%>%
  mutate(DT_round = with_tz(DT_round, tzone = "UTC"),
         last_site_visit = with_tz(last_site_visit, tzone = "UTC"),
         DT_join = as.character(DT_round))

sensor_malfunction_notes <- grab_mWater_malfunction_notes(mWater_api_data = mWater_data)%>%
  mutate(start_DT = with_tz(start_DT, tzone = "UTC"),
         end_DT = with_tz(end_DT, tzone = "UTC"))
```

## Pulling in the sensor data from hydrovu

## Custom API puller Function

```{r}
api_puller <- function(site, 
                       start_dt, end_dt = Sys.time(), 
                       api_token,
                       hv_sites_arg = hv_sites,
                       dump_dir, 
                       synapse_env = FALSE, fs = NULL) {
  
  # Synapse runs in parallel, so stagger API calls to prevent overloading server
  Sys.sleep(runif(1, 2, 5))
  
  # For other sites, filter locations that contain the site name
  site_loc <- hv_sites_arg %>%
    dplyr::mutate(name = tolower(name)) %>%
    dplyr::filter(grepl(site, name, ignore.case = TRUE))
  
  # Request data for each location ID within the specified time period
  all_data_filtered <- purrr::map(site_loc$id, # Extract the HydroVu location IDs for API requests
                                  function(id){
                                    hv_data_id(loc_id = id,
                                               start_time = start_dt,
                                               end_time = end_dt,
                                               token = api_token,
                                               tz = "UTC")}) %>% 
    # Filter out error responses (404s) and keep only valid data frames
    purrr::keep(., is.data.frame)
  
  # If no data was found for this site during the time period, report, end current iteration, and continue
  if(length(all_data_filtered) == 0){
    message(paste0("No data at ", site, " during this time frame"))
    return() # This will end the current iteration and move to the next one
  } 
  
  # Combine all dataframes, standardize column names, and join with location metadata
  site_df <- dplyr::bind_rows(all_data_filtered) %>%
    data.table::data.table() %>%
    dplyr::rename(id = Location,
                  parameter = Parameter,
                  units = Units) %>%
    dplyr::left_join(., site_loc, by = "id") %>%
    dplyr::mutate(site = tolower(site)) %>%
    dplyr::select(site, id, name, timestamp, parameter, value, units)
  
  # Format the timestamp string for filenames
  timestamp_str <- format(end_dt, "%Y%m%d-T%H%M%SZ", tz = "UTC")
  # Create clean path (remove any double slashes) for file upload
  file_name <- paste0(site, timestamp_str, ".parquet")
  file_path <- file.path(dump_dir, file_name)
  
  if (synapse_env){
    # Upload to ADLS via ADLS-compatible write procedure
    tryCatch({
      # First write to a temporary file
      temp_file <- tempfile(fileext = ".parquet")
      arrow::write_parquet(site_df, temp_file)
      
      # Upload file to ADLS 
      AzureStor::upload_adls_file(
        filesystem = fs, 
        src = temp_file,
        dest = file_path)
      message("...Upload complete for: ", site, "\n")
    }, error = function(e) {
      message("...Error in upload process: ", e$message, "\n")
    })
  } else {
    arrow::write_parquet(site_df, file_path)
    message(paste("...Upload into", dump_dir, "complete for:", site, "\n"))
  }
}
```

## HydroVu Pull

```{r}

hv_sites <- hv_locations_all(hv_token) %>%
  filter(!grepl("vulink", name, ignore.case = TRUE))%>%
    filter(!grepl("2024", name, ignore.case = TRUE))


walk(sites,
     function(site) {
       message("Requesting HV data for: ", site)
       api_puller(
         site = site,
         start_dt = with_tz(mst_start, tzone = "UTC"),
         end_dt = with_tz(mst_end, tzone = "UTC"),
         api_token = hv_token,
         hv_sites_arg = hv_sites,
         dump_dir = staging_directory
       )
     }
)

# beep to let you know when it's done
beepr::beep(1)
```

## Pulling in any downloaded log data

Some logs (Vulink Log if sonde is AT600/800) cannot be backed up to HydroVu. If you have any of these files, you can read them in here.

```{r}
log_files <- list.files(log_directory, pattern = "\\vulink.html$", full.names = TRUE)%>%
  #only keep files with our sites in them
  keep(~any(str_detect(tolower(.x), sites)))

log_files_simp <- log_files%>%
  str_replace(log_directory, "")%>%
  str_replace("/", "")%>%
  str_replace_all("\\.html", "")%>%
  str_squish()

html_content <- map(log_files, ~rvest::read_html(.x))

log_data <- map(html_content, ~parse_insitu_html_log(.x))

names(log_data) <- log_files_simp

cleaned_log_data <- log_data%>%
  rbindlist()%>%
  #fixing a few specific location names before passing to fix_site_names
  mutate( site = tolower(site),
          id = as.double(sensor_sn),
         timestamp = with_tz(DT, tzone = "UTC"), 
         site = case_when(site == "archery range"~ "archery", 
                         site == "boxelder creek" ~ "boxcreek", 
                         TRUE ~ site), 
         name = paste0("Vulink Log - ", site))%>%
  fix_site_names()%>%
  select(site, id, name, timestamp, parameter, value, units = unit) %>%
  distinct(.keep_all = TRUE) %>%
  split(f = list(.$site), sep = "-") %>%
  keep(~nrow(.) > 0)

#write out the log data to the staging directory so it can be processed with the HV data
iwalk(cleaned_log_data, ~write_parquet(.x, here(staging_directory, paste0(.y, "_log_data.parquet"))))

# beep to let you know when it's done
beepr::beep(1)

```


# Raw Data Processing
## Load in and tidy the raw files

```{r}
#Since we no longer have any virridy sites in the network, we can just use a normal munge!
hv_data <- munge_api_data(api_dir = staging_directory,synapse_env = F)%>%
  # split into site-parameter combinations for parallel processing
  split(f = list(.$site, .$parameter), sep = "-") %>%
  keep(~nrow(.) > 0)

tidy_data <- hv_data %>%
  future_map(~tidy_api_data(api_data = .), .progress = TRUE, .options = set_furr_options) %>%  # the summarize interval default is 15 minutes
  keep(~!is.null(.))%>%
  # Only keep parameters we are interested in
   keep(~unique(.$parameter) %in% c("Specific Conductivity", "Temperature", "DO", "pH", "ORP", "Depth", #Standard vars
                                    "Chl-a Fluorescence", "Turbidity", "FDOM Fluorescence")) # Optical Vars

```

## Combine with field notes & add summary stats

```{r}
# Add the field note data to all of the data
# Quick name fix for mountain campus
all_field_notes <- all_field_notes %>%
  fix_site_names() %>%
  mutate(site = ifelse(site == "mountaincampus", "mtncampus", site))

combined_data <- tidy_data %>%
  future_map(~add_field_notes(df = ., notes = all_field_notes), .progress = TRUE)

# Add summary statistics
summarized_data <- combined_data %>%
  map(~generate_summary_statistics(.))

# beep to let you know when it's done
beepr::beep(1)
```

# Data Flagging

## Single Sensor Flags

```{r}
# Chunk the data for furrr
summarized_data_chunks <- split(1:length(summarized_data),
                                ceiling(seq_along(1:length(summarized_data))/10))
# Flag data...
# Single parameter flags
single_sensor_flags <- list()
for (chunk_idx in seq_along(summarized_data_chunks)) {
  message("\n=== Processing chunk ", chunk_idx, " of ", length(summarized_data_chunks), " ===")

  # Get the indices for this chunk
  indices <- summarized_data_chunks[[chunk_idx]]
  chunk_data <- summarized_data[indices]

  # Process the chunk in parallel
  chunk_results <- chunk_data %>%
    future_map(
      function(data) {
        flagged_data <- data %>%
          data.table(.) %>%
          # flag field visits
          add_field_flag(df = .) %>%
          # flag missing data
          add_na_flag(df = .) %>%
          # flag DO noise
          find_do_noise(df = .) %>%
          # flag repeating values
          add_repeat_flag(df = .) %>%
          # find times when sonde was moved up/down in housing
          add_depth_shift_flag(df = ., level_shift_table =  all_field_notes, post2024 = TRUE) %>%
          # find instances of sensor drift (FDOM, Chl-a, Turbidity only)
          add_drift_flag(df = .)

        if (unique(data$parameter) %in% names(sensor_thresholds)) {
          # flag instances outside the spec range
          flagged_data <- flagged_data %>%
            data.table(.) %>%
            add_spec_flag(df = ., spec_table = sensor_thresholds)
        }

        if (unique(data$parameter) %in% unique(season_thresholds$parameter)) {
          # flag instances outside the spec range
          flagged_data <- flagged_data %>%
            data.table(.) %>%
            add_seasonal_flag(df = ., threshold_table = season_thresholds)
        }

        flagged_data <- flagged_data %>%
          data.table(.)

        return(flagged_data)
      },
      .progress = TRUE, .options = set_furr_options
    )

  # Add chunk to list
  single_sensor_flags <- c(single_sensor_flags, chunk_results)

  if (chunk_idx < length(summarized_data_chunks)) {
    message("Taking a short break before next chunk...")
    gc()
    Sys.sleep(0.1)
  }
}

# beep to let you know when it's done
beepr::beep(1)

```

## Intrasensor flags

```{r}

intrasensor_flags <- single_sensor_flags %>%
  rbindlist(fill = TRUE) %>%
  split(by = "site")

# Chunk the data for furrr
intrasensor_data_chunks <- split(1:length(intrasensor_flags),
                                 ceiling(seq_along(1:length(intrasensor_flags))/3))

intrasensor_flags_list <- list()
for (chunk_idx in seq_along(intrasensor_data_chunks)) {
  message("\n=== Processing chunk ", chunk_idx, " of ", length(intrasensor_data_chunks), " ===")

  # Get the indices for this chunk
  indices <- intrasensor_data_chunks[[chunk_idx]]
  chunk_data <- intrasensor_flags[indices]
  # Process the chunk in parallel
  chunk_results <- chunk_data %>%
    future_map(
      function(data) {
        # A chunk is a site df
        flagged_data <- data %>%
          data.table() %>%
          # flag times when water was below freezing
          add_frozen_flag(.) %>%
          # overflagging correction. remove slope violation flag if it occurs concurrently
          # with temp or depth
          intersensor_check(.) %>%
          # add sonde burial. If DO is noise is long-term, likely burial:
          add_burial_flag(.) %>%
          # flag times when sonde was unsubmerged
          add_unsubmerged_flag(.)

        return(flagged_data)
      }, .progress = TRUE, .options = set_furr_options
    ) %>%
    rbindlist(fill = TRUE) %>%
    # lil' cleanup of flag column contents
    dplyr::mutate(flag = ifelse(flag == "", NA, flag)) %>%
    # transform back to site-parameter dfs
    split(f = list(.$site, .$parameter), sep = "-") %>%
    purrr::discard(~ nrow(.) == 0) %>%
    # Add in KNOWN instances of sensor malfunction
    map(~add_malfunction_flag(df = ., malfunction_records = sensor_malfunction_notes))

  # Add chunk to list
  intrasensor_flags_list <- c(intrasensor_flags_list, chunk_results)

  if (chunk_idx < length(intrasensor_data_chunks)) {
    message("Taking a short break before next chunk...")
    gc()
    Sys.sleep(0.1)
  }
}

# beep to let you know when it's done
beepr::beep(1)

```

### Let's temporarily save this data so i can remove everything else from memory

```{r}
iwalk(intrasensor_flags_list, ~write_parquet(.x, here(flagged_temp_dir, paste0(.y, ".parquet"))))

#remove everything other than intrasensor flags from memory
rm(list=setdiff(ls(), "intrasensor_flags_list"))
#clean up memory
gc()

```

## Applying network check
```{r}

# Because we are pulling in all of the data for all of the sites, and the
# network check to do that is not applicable, here we make a custom net work check function

site_order_2025 <- function(site_name){
  # Primary network order
  sites_order <-  c("joei",
                    "cbri",
                    "chd",
                    "pfal",
                    "pbr",
                    "pman",
                    "pbd",
                    "bellvue",
                    "salyer",
                    "udall",
                    "riverbend",
                    "cottonwood",
                    "elc",
                    "archery",
                    "riverbluffs")
  # If the site is in the primary order, return that order
  if(site_name %in% sites_order){
    return(sites_order)
  }
  # springcreek cases
  if(site_name == "springcreek"){
    sites_order <- c("riverbend",
                     "springcreek",
                     "cottonwood")
    return(sites_order)
  }
  # boxcreek cases
  if(site_name == "boxcreek"){
    sites_order <- c("elc",
                     "boxcreek",
                     "archery")
     return(sites_order)
  }
  # disconnected/independant sites: mtn campus, sfm
  if (site_name %in% c("mtncampus", "sfm")){
    return(NULL)
  }
}


custom_network_check <- function(df, intrasensor_flags_arg = intrasensor_flags_list, site_order_fun) {
  
  site_name <- unique(na.omit(df$site))
  parameter_name <- unique(na.omit(df$parameter))
  
  sites_order <- site_order_fun(site_name)
  # If site is not in a defined order, return original df
    if (is.null(sites_order)) {
    return(df)  # nothing to do
  }

  # Find the index of current site in ordered list
  site_index <- which(sites_order == sites_order[grep(site_name, sites_order, ignore.case = TRUE)])

  # Create site-parameter identifier
  site_param <- paste0(site_name, "-", parameter_name)

  # Initialize empty dataframes for upstream/downstream sites
  upstr_site_df <- tibble(DT_round = NA)
  dnstr_site_df <- tibble(DT_round = NA)

  # Try to get upstream site data
  tryCatch({
    # Skip trying to find upstream sites for first sites
    if (site_index != 1){
      previous_site <- paste0(sites_order[site_index-1], "-", parameter_name)
      upstr_site_df <- intrasensor_flags_arg[[previous_site]] %>%
        select(DT_round, site_up = site, flag_up = flag) %>%
        data.table()
    }
  },
  error = function(err) {
    message(paste0("No UPSTREAM data found for ", site_param, ". Expected at site '", previous_site, "'."))
  })

  # Try to get downstream site data
  tryCatch({
    # Skip trying to find downstream sites for first sites
    if (site_index != length(sites_order)){
      next_site <- paste0(sites_order[site_index+1], "-", parameter_name)
      dnstr_site_df <- intrasensor_flags_arg[[next_site]] %>%
        select(DT_round, site_down = site, flag_down = flag) %>%
        data.table()
    }
  },
  error = function(err) {
    message(paste0("No DOWNSTREAM data found for ", site_param, ". Expected at site '", next_site, "'."))
  })

  # Join current site data with upstream and downstream data
  up_down_join <- df %>%
    left_join(upstr_site_df, by = "DT_round") %>%
    left_join(dnstr_site_df, by = "DT_round")

  # Helper functions
  # Function to add column if it doesn't exist
  add_column_if_not_exists <- function(df, column_name, default_value = NA) {
    if (!column_name %in% colnames(df)) {
      df <- df %>% dplyr::mutate(!!sym(column_name) := default_value)
    }
    return(df)
  }

  # Function to check if any flags exist in a time window
  check_2_hour_window_fail <- function(x) {
    sum(x) >= 1
  }

  # Establish helper objects
  # String object that is used to ignore flags that we do not want to remove.
  ignore_flags <- "drift|DO interference|repeat|sonde not employed|frozen|
  unsubmerged|missing data|site visit|sv window|sensor malfunction|burial|
  sensor biofouling|improper level cal|sonde moved"

  # Numeric object that determines the width for the rolling window check (2 hours)
  width_fun = 17

  # Process flags based on upstream/downstream patterns
  final_df <- up_down_join %>%
    # Add placeholder columns if joinging didn't provide them
    add_column_if_not_exists("flag_down") %>%
    add_column_if_not_exists("flag_up") %>%
    add_column_if_not_exists("site_down") %>%
    add_column_if_not_exists("site_up") %>%
    # Create binary indicator for upstream/downstream flags
    ## 0 = no relevant flags upstream/downstream
    ## 1 = at least one site has relevant flags
    dplyr::mutate(flag_binary = dplyr::if_else(
      (is.na(flag_up) | grepl(ignore_flags, flag_up)) &
        (is.na(flag_down) | grepl(ignore_flags, flag_down)), 0, 1
    )) %>%
    # Check for flags in 4-hour window (+/-2 hours around each point, 17 observations at 15-min intervals)
    dplyr::mutate(overlapping_flag = zoo::rollapply(flag_binary, width = width_fun, FUN = check_2_hour_window_fail, fill = NA, align = "center")) %>%
    add_column_if_not_exists(column_name = "auto_flag") %>%
    # If flag exists but is also present up/downstream, it likely represents a real event
    # In that case, remove the flag (set auto_flag to NA)
    dplyr::mutate(auto_flag = ifelse(!is.na(flag) & !grepl(ignore_flags, flag) &
                                       (overlapping_flag == TRUE & !is.na(overlapping_flag)), NA, flag)) %>%
    dplyr::select(-c(flag_up, flag_down, site_up, site_down, flag_binary, overlapping_flag))

  return(final_df)
}

# I really don't understand how network check would work in parallel, so I don't.
final_flags <- intrasensor_flags_list %>%
  purrr::map(~custom_network_check(df = ., intrasensor_flags_arg = intrasensor_flags_list, site_order_fun = site_order_2025)) %>%
  rbindlist(fill = TRUE) %>%
  tidy_flag_column() %>%
  split(f = list(.$site, .$parameter), sep = "-") %>%
  purrr::map(~add_suspect_flag(.)) %>%
  rbindlist(fill = TRUE)

v_final_flags <- final_flags%>%
  dplyr::mutate(auto_flag = ifelse(is.na(auto_flag), NA,
                                   ifelse(auto_flag == "suspect data" & is.na(lag(auto_flag, 1)) & is.na(lead(auto_flag, 1)), NA, auto_flag))) %>%
  dplyr::select(c("DT_round", "DT_join", "site", "parameter", "mean", "units", "n_obs", "spread", "auto_flag", "mal_flag", "sonde_moved","sonde_employed", "season", "last_site_visit")) %>%
  dplyr::mutate(auto_flag = ifelse(is.na(auto_flag), NA, ifelse(auto_flag == "", NA, auto_flag))) %>%
  split(f = list(.$site, .$parameter), sep = "-") %>%
  keep(~nrow(.) > 0)

# beep to let you know when it's done
beepr::beep(1)

```

# Save Final Flagged Dataset

```{r}

# Save the data individually.
iwalk(v_final_flags, ~write_csv(.x, here(flagged_directory, paste0(.y, ".csv"))))

```
