---
title: "Process_2019_2021"
author: "Katie Willi"
date: "2025-09-27"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This is the workflow for organizing old (2019-2021) Poudre Sonde Network data. This data was collected before our current protocol was produced, and therefore requires a slightly different workflow for performing our auto-QAQC pipeline. 

## 1) Packages & parallel configuration

```{r}
# load required packages
package_loader <- function(x) {
  if (x %in% installed.packages()) {
    suppressMessages({
      library(x, character.only = TRUE)
    })
  } else {
    suppressMessages({
      install.packages(x)
      library(x, character.only = TRUE)
    })
  }
}

# load all required packages
invisible(
  lapply(c("arrow", "data.table", "httr2", "tidyverse", "lubridate", "zoo",
           "padr", "stats", "RcppRoll", "yaml", "here", "furrr", "fcw.qaqc"),
         package_loader)
)


# Configure parallel processing
max_workers <- 4  # Maximum number of parallel workers
num_workers <- min(availableCores() - 1, max_workers)
plan(multisession, workers = num_workers)
furrr_options(
  globals = TRUE,
  packages = c("arrow", "data.table", "httr2", "tidyverse", "lubridate", "zoo",
               "padr", "stats", "RcppRoll", "yaml", "here", "fcw.qaqc")
)

```

## 2) Thresholds, credentials, and field notes data

```{r}
# Read in the threshold data first
sensor_thresholds <- read_yaml(here("data","field_notes","qaqc",  "sensor_spec_thresholds.yml"))
season_thresholds <- read_csv(here("data","field_notes","qaqc", "updated_seasonal_thresholds_2025_sjs.csv"), show_col_types = FALSE) %>%
  fix_site_names()

# Configure your credentials files
mwater_creds_file <- "creds/mWaterCreds.yml"
mWater_creds <- read_yaml(mwater_creds_file)

# pull field data from mWater API
mWater_data <- load_mWater(creds = mWater_creds)

# grab field notes with proper timezone handling
all_field_notes <- grab_mWater_sensor_notes(mWater_api_data = mWater_data) %>%
  mutate(DT_round = with_tz(DT_round, tzone = "UTC"),
         last_site_visit = with_tz(last_site_visit, tzone = "UTC"),
         DT_join = as.character(DT_round))

# grab sensor malfunction records
sensor_malfunction_notes <- grab_mWater_malfunction_notes(mWater_api_data = mWater_data) %>%
  mutate(start_DT = with_tz(start_DT, tzone = "UTC"),
         end_DT = with_tz(end_DT, tzone = "UTC"))
```

## 3) Ingest & standardize raw data

```{r}
# HTML readers, one for HydroVu htmls, one for direct downloads from the AquaTROLL 600, and one for downloading from the Vulink in the field:

# from HydroVu:
hydrovu_reader <- function(file) {
  
  raw_data <- rvest::read_html(file) %>%
    rvest::html_node('table') %>%
    rvest::html_table() %>%
    slice(-1:-8) %>%
    janitor::row_to_names(row_number = 1) 
}

# from AquaTROLL 600:
troll_reader <- function(file) {
  
  raw_data <- rvest::read_html(file) %>%
    rvest::html_node('table') %>%
    rvest::html_table() %>%
    slice(-1:-25) %>%
    janitor::row_to_names(row_number = 1) 
}

# from VuLink in the field:
vulink_reader <- function(file) {
  
  raw_data <- rvest::read_html(file) %>%
    rvest::html_node('table') %>%
    rvest::html_table() %>%
    slice(-1:-31) %>%
    janitor::row_to_names(row_number = 1) 
}
```


```{r}
## Rist

hydrovu_utc <- map_dfr(list.files("data/sensor_data/2018_2021/site_organized_hydrovu/rist/", full.names = T), hydrovu_reader) %>%
  select(DT = `Date Time`,
         Water_Temp_C = `Temperature (C)`,
         pH = `pH (pH)`,
         ORP_mV = `ORP (mV)`,
         Actual_Conductivity_µS_cm = `Actual Conductivity (µS/cm)`,
         Specific_Conductivity_µS_cm = `Specific Conductivity (µS/cm)`,
         Salinity_PSU = `Salinity (psu)`,
         TDS_ppt = `Total Dissolved Solids (ppt)`,
         DO_ppm = `DO (ppm)`,
         Perc_O2_Sat = `% Saturation O₂ (% sat)`,
         Turbidity_NTU = `Turbidity (FNU)`,
         TSS_mg_L = `Total Suspended Solids (mg/L)`,
         #Chla_RFU = Chl.a.Fluorescence..RFU.,
         #Chla_µg_L = Chl.a.Concentration..µg.L.,
         Pressure_PSI = `Pressure (psi)`,
         Depth_ft = `Depth (ft)`,
         # Elevation_m = `Level: Elevation (m)`,
         Depth_to_Water_ft = `Level: Depth to Water (ft)`) %>%
  mutate(DT = as_datetime(DT, tz = "UTC")) 

rist <- hydrovu_utc %>%
  mutate(year = lubridate::year(DT)) %>%
  mutate(date = as_date(substr(DT, 1, 10))) %>%
  distinct(.keep_all = TRUE) %>%
  # Force time zones appropriately. MST to UTC:
  mutate(DT = case_when(year %in% c(2019, 2020) ~ DT + hours(7),
                        TRUE ~ DT)) %>% 
  mutate(sensor = "rist",
         comments = NA)

rm(hydrovu_utc)

## Legacy

hydrovu_utc <- map_dfr(list.files("data/sensor_data/2018_2021/site_organized_hydrovu/legacy/", pattern = "hydrovu", full.names = T), hydrovu_reader) %>%
  select(DT = `Date Time`,
         Water_Temp_C = `Temperature (C)`,
         pH = `pH (pH)`,
         ORP_mV = `ORP (mV)`,
         Actual_Conductivity_µS_cm = `Actual Conductivity (µS/cm)`,
         Specific_Conductivity_µS_cm = `Specific Conductivity (µS/cm)`,
         Salinity_PSU = `Salinity (psu)`,
         TDS_ppt = `Total Dissolved Solids (ppt)`,
         DO_ppm = `DO (ppm)`,
         Perc_O2_Sat = `% Saturation O₂ (% sat)`,
         Turbidity_NTU = `Turbidity (FNU)`,
         TSS_mg_L = `Total Suspended Solids (mg/L)`,
         #Chla_RFU = Chl.a.Fluorescence..RFU.,
         #Chla_µg_L = Chl.a.Concentration..µg.L.,
         Pressure_PSI = `Pressure (psi)`,
         Depth_ft = `Depth (ft)`,
         # Elevation_m = `Level: Elevation (m)`,
         Depth_to_Water_ft = `Level: Depth to Water (ft)`) %>%
  mutate(DT = as_datetime(DT, tz = "UTC"))

troll_mdt <- map_dfr(list.files("data/sensor_data/2018_2021/site_organized_hydrovu/legacy/", full.names = T, pattern = "troll_mdt"), troll_reader)
names(troll_mdt) <- make.names(gsub("[[:digit:]]", "", names(troll_mdt)), unique = T)
troll_mdt <- troll_mdt %>%
  select(DT = Date.Time,
         Water_Temp_C = Temperature...C....,
         pH = pH..pH....,
         ORP_mV = ORP..mV....,
         Actual_Conductivity_µS_cm = Actual.Conductivity..µS.cm....,
         Specific_Conductivity_µS_cm = Specific.Conductivity..µS.cm....,
         Salinity_PSU = Salinity..PSU....,
         TDS_ppt = Total.Dissolved.Solids..ppt....,
         DO_ppm = RDO.Concentration..mg.L....,
         Perc_O2_Sat = RDO.Saturation...Sat....,
         Turbidity_NTU = Turbidity..NTU....,
         TSS_mg_L = Total.Suspended.Solids..mg.L....,
         Pressure_PSI = Pressure..psi....,
         Depth_ft = Depth..ft....) %>%
  mutate(DT_local = force_tz(ymd_hms(DT), tzone = "America/Denver"),
         DT = with_tz(DT_local, tzone = "UTC")) %>%
  filter(DT_local >= "2021-07-25 00:00:00" & DT_local <="2021-08-25 23:00:00") 

vulink_mdt <-  map_dfr(list.files("data/sensor_data/2018_2021/site_organized_hydrovu/legacy/", full.names = T, pattern = "vulink_mdt"), vulink_reader) %>% select(-2:-4)
names(vulink_mdt) <- make.names(gsub("[[:digit:]]", "", names(vulink_mdt)), unique = T)
vulink_mdt <- vulink_mdt %>%
  select(DT = Date.Time,
         Water_Temp_C = Temperature...C....,
         pH = pH..pH....,
         ORP_mV = ORP..mV....,
         Actual_Conductivity_µS_cm = Actual.Conductivity..µS.cm....,
         Specific_Conductivity_µS_cm = Specific.Conductivity..µS.cm....,
         Salinity_PSU = Salinity..PSU....,
         TDS_ppt = Total.Dissolved.Solids..ppt....,
         DO_ppm = RDO.Concentration..mg.L....,
         Perc_O2_Sat = RDO.Saturation...Sat....,
         Turbidity_NTU = Turbidity..NTU....,
         TSS_mg_L = Total.Suspended.Solids..mg.L....,
         Pressure_PSI = Pressure..psi....,
         Depth_ft = Depth..ft....) %>%
  mutate(DT_local = force_tz(ymd_hms(DT), tzone = "America/Denver"),
         DT = with_tz(DT_local, tzone = "UTC")) %>%
  filter(DT_local < "2021-07-25 00:00:00" | DT_local >"2021-08-25 23:00:00")

troll_mst <- map_dfr(list.files("data/sensor_data/2018_2021/site_organized_hydrovu/legacy/", full.names = T, pattern = "troll_mst"), troll_reader)
names(troll_mst) <- make.names(gsub("[[:digit:]]", "", names(troll_mst)), unique = T)
troll_mst <- troll_mst %>% 
  select(DT = Date.Time,
         Water_Temp_C = Temperature...C....,
         pH = pH..pH....,
         ORP_mV = ORP..mV....,
         Actual_Conductivity_µS_cm = Actual.Conductivity..µS.cm....,
         Specific_Conductivity_µS_cm = Specific.Conductivity..µS.cm....,
         Salinity_PSU = Salinity..PSU....,
         TDS_ppt = Total.Dissolved.Solids..ppt....,
         DO_ppm = RDO.Concentration..mg.L....,
         Perc_O2_Sat = RDO.Saturation...Sat....,
         Turbidity_NTU = Turbidity..NTU....,
         #TSS_mg_L = Total.Suspended.Solids..mg.L....,
         Pressure_PSI = Pressure..psi....,
         Depth_ft = Depth..ft....) %>%
  mutate(DT_local = force_tz(parse_date_time(DT, orders = c("ymd HMS", "ymd HM", "mdy HMS", "mdy HM")), tzone = "MST7MDT"),
         DT = with_tz(DT_local + hours(1), "UTC"))

legacy <- bind_rows(hydrovu_utc, troll_mdt, vulink_mdt, troll_mst) %>%
  # mutate(DT = round_date(DT, "15 minutes")) %>%
  mutate(year = lubridate::year(DT)) %>%
  mutate(date = as_date(substr(DT, 1, 10))) %>%
  distinct(.keep_all=TRUE) %>%
  # Force time zones appropriately. MDT to UTC:
  mutate(DT = case_when(year %in% c(2019, 2020) ~ DT + hours(6),
                        TRUE ~ DT)) %>% 
  mutate(sensor = "legacy", comments = NA) %>%
  select(-DT_local)

rm(hydrovu_utc, troll_mdt, troll_mst, vulink_mdt)

## Timberline

hydrovu_utc <- map_dfr(list.files("data/sensor_data/2018_2021/site_organized_hydrovu/timberline/", full.names = T), hydrovu_reader) %>%
  select(DT = `Date Time`,
         Water_Temp_C = `Temperature (C)`,
         pH = `pH (pH)`,
         ORP_mV = `ORP (mV)`,
         Actual_Conductivity_µS_cm = `Actual Conductivity (µS/cm)`,
         Specific_Conductivity_µS_cm = `Specific Conductivity (µS/cm)`,
         Salinity_PSU = `Salinity (psu)`,
         TDS_ppt = `Total Dissolved Solids (ppt)`,
         DO_ppm = `DO (ppm)`,
         Perc_O2_Sat = `% Saturation O₂ (% sat)`,
         Turbidity_NTU = `Turbidity (FNU)`,
         TSS_mg_L = `Total Suspended Solids (mg/L)`,
         #Chla_RFU = Chl.a.Fluorescence..RFU.,
         #Chla_µg_L = Chl.a.Concentration..µg.L.,
         Pressure_PSI = `Pressure (psi)`,
         Depth_ft = `Depth (ft)`,
         # Elevation_m = `Level: Elevation (m)`,
         Depth_to_Water_ft = `Level: Depth to Water (ft)`) %>%
  mutate(DT = as_datetime(DT, tz = "UTC")) 

timberline <- hydrovu_utc %>% 
  # mutate(DT = round_date(DT, "15 minutes")) %>%
  mutate(year = lubridate::year(DT)) %>%
  mutate(date = as_date(substr(DT, 1, 10))) %>%
  distinct(.keep_all = TRUE) %>%
  # Force time zones appropriately. MST to UTC:
  mutate(DT = if_else(year == 2020, DT + hours(7), DT)) %>% 
  mutate(sensor = "timberline",
         comments = NA)

rm(hydrovu_utc)

## Prospect

hydrovu_utc <- map_dfr(list.files("data/sensor_data/2018_2021/site_organized_hydrovu/prospect", full.names = T), hydrovu_reader) %>%
  select(DT = `Date Time`,
         Water_Temp_C = `Temperature (C)`,
         pH = `pH (pH)`,
         ORP_mV = `ORP (mV)`,
         Actual_Conductivity_µS_cm = `Actual Conductivity (µS/cm)`,
         Specific_Conductivity_µS_cm = `Specific Conductivity (µS/cm)`,
         Salinity_PSU = `Salinity (psu)`,
         TDS_ppt = `Total Dissolved Solids (ppt)`,
         DO_ppm = `DO (ppm)`,
         Perc_O2_Sat = `% Saturation O₂ (% sat)`,
         Turbidity_NTU = `Turbidity (FNU)`,
         TSS_mg_L = `Total Suspended Solids (mg/L)`,
         #Chla_RFU = Chl.a.Fluorescence..RFU.,
         #Chla_µg_L = Chl.a.Concentration..µg.L.,
         Pressure_PSI = `Pressure (psi)`,
         Depth_ft = `Depth (ft)`,
         # Elevation_m = `Level: Elevation (m)`,
         Depth_to_Water_ft = `Level: Depth to Water (ft)`) %>%
  mutate(DT = as_datetime(DT, tz = "UTC")) 

prospect <- hydrovu_utc %>%
  # mutate(DT = round_date(DT, "15 minutes")) %>%
  mutate(year = lubridate::year(DT)) %>%
  mutate(date = as_date(substr(DT, 1, 10))) %>%
  distinct(.keep_all=TRUE) %>%
  # Force time zones appropriately. MDT to UTC:
  mutate(DT = case_when(date < "2020-09-01" ~ DT + hours(6),
                        # MST to UTC:
                        date >= "2020-09-01" ~ DT + hours(7),
                        TRUE ~ DT)) %>% 
  mutate(sensor = "prospect",
         comments = NA)

rm(hydrovu_utc)

## ELC

hydrovu_utc <- map_dfr(list.files("data/sensor_data/2018_2021/site_organized_hydrovu/elc/", full.names = T, pattern = "hydrovu_utc"), hydrovu_reader) %>%
  select(DT = `Date Time`,
         Water_Temp_C = `Temperature (C)`,
         pH = `pH (pH)`,
         ORP_mV = `ORP (mV)`,
         Actual_Conductivity_µS_cm = `Actual Conductivity (µS/cm)`,
         Specific_Conductivity_µS_cm = `Specific Conductivity (µS/cm)`,
         Salinity_PSU = `Salinity (psu)`,
         TDS_ppt = `Total Dissolved Solids (ppt)`,
         DO_ppm = `DO (ppm)`,
         Perc_O2_Sat = `% Saturation O₂ (% sat)`,
         Turbidity_NTU = `Turbidity (FNU)`,
         TSS_mg_L = `Total Suspended Solids (mg/L)`,
         #Chla_RFU = Chl.a.Fluorescence..RFU.,
         #Chla_µg_L = Chl.a.Concentration..µg.L.,
         Pressure_PSI = `Pressure (psi)`,
         Depth_ft = `Depth (ft)`,
         # Elevation_m = `Level: Elevation (m)`,
         Depth_to_Water_ft = `Level: Depth to Water (ft)`) %>%
  mutate(DT = as_datetime(DT, tz = "UTC")) 

hydrovu_mst <- map_dfr(list.files("data/sensor_data/2018_2021/site_organized_hydrovu/elc/", full.names = T, pattern = "hydrovu_mst"), hydrovu_reader) %>%
  select(DT = `Date Time`,
         Water_Temp_C = `Temperature (C)`,
         pH = `pH (pH)`,
         ORP_mV = `ORP (mV)`,
         Actual_Conductivity_µS_cm = `Actual Conductivity (µS/cm)`,
         Specific_Conductivity_µS_cm = `Specific Conductivity (µS/cm)`,
         Salinity_PSU = `Salinity (psu)`,
         TDS_ppt = `Total Dissolved Solids (ppt)`,
         DO_ppm = `DO (ppm)`,
         Perc_O2_Sat = `% Saturation O₂ (% sat)`,
         Turbidity_NTU = `Turbidity (FNU)`,
         TSS_mg_L = `Total Suspended Solids (mg/L)`,
         #Chla_RFU = Chl.a.Fluorescence..RFU.,
         #Chla_µg_L = Chl.a.Concentration..µg.L.,
         Pressure_PSI = `Pressure (psi)`,
         Depth_ft = `Depth (ft)`) %>%
  mutate(DT_local = force_tz(parse_date_time(DT, orders = c("ymd HMS", "ymd HM", "mdy HMS", "mdy HM")), tzone = "MST7MDT"),
         DT = with_tz(DT_local + hours(1), "UTC"))

troll_mdt <- map_dfr(list.files("data/sensor_data/2018_2021/site_organized_hydrovu/elc/", full.names = T, pattern = "troll_mdt"), troll_reader)
names(troll_mdt) <- make.names(gsub("[[:digit:]]", "", names(troll_mdt)), unique = T)
troll_mdt <- troll_mdt %>%
  select(DT = Date.Time,
         Water_Temp_C = Temperature...C....,
         pH = pH..pH....,
         ORP_mV = ORP..mV....,
         Actual_Conductivity_µS_cm = Actual.Conductivity..µS.cm....,
         Specific_Conductivity_µS_cm = Specific.Conductivity..µS.cm....,
         Salinity_PSU = Salinity..PSU....,
         TDS_ppt = Total.Dissolved.Solids..ppt....,
         DO_ppm = RDO.Concentration..mg.L....,
         Perc_O2_Sat = RDO.Saturation...Sat....,
         Turbidity_NTU = Turbidity..NTU....,
         TSS_mg_L = Total.Suspended.Solids..mg.L....,
         Pressure_PSI = Pressure..psi....,
         Depth_ft = Depth..ft....) %>%
  mutate(DT_local = force_tz(ymd_hms(DT), tzone = "America/Denver"),
         DT = with_tz(DT_local, tzone = "UTC"))

elc <- bind_rows(hydrovu_utc, hydrovu_mst, troll_mdt) %>%
  # mutate(DT = round_date(DT, "15 minutes")) %>%
  mutate(year = lubridate::year(DT)) %>%
  mutate(date = as_date(substr(DT, 1, 10))) %>%
  distinct(.keep_all=TRUE) %>%
  mutate(DT = if_else(year == 2019, DT + hours(6), DT)) %>%
  mutate(sensor = "elc",
         comments = NA) %>%
  select(-DT_local)

rm(hydrovu_mst, hydrovu_utc, troll_mdt)

## Archery

hydrovu_utc <- map_dfr(list.files("data/sensor_data/2018_2021/site_organized_hydrovu/archery/", full.names = T), hydrovu_reader) %>%
  select(DT = `Date Time`,
         Water_Temp_C = `Temperature (C)`,
         pH = `pH (pH)`,
         ORP_mV = `ORP (mV)`,
         Actual_Conductivity_µS_cm = `Actual Conductivity (µS/cm)`,
         Specific_Conductivity_µS_cm = `Specific Conductivity (µS/cm)`,
         Salinity_PSU = `Salinity (psu)`,
         TDS_ppt = `Total Dissolved Solids (ppt)`,
         DO_ppm = `DO (ppm)`,
         Perc_O2_Sat = `% Saturation O₂ (% sat)`,
         Turbidity_NTU = `Turbidity (FNU)`,
         TSS_mg_L = `Total Suspended Solids (mg/L)`,
         Chla_RFU = `Chl-a Fluorescence (RFU)`,
         Chla_µg_L = `Chl-a Concentration (µg/L)`,
         Pressure_PSI = `Pressure (psi)`,
         Depth_ft = `Depth (ft)`,
         # Elevation_m = `Level: Elevation (m)`,
         Depth_to_Water_ft = `Level: Depth to Water (ft)`) %>%
  mutate(DT = as_datetime(DT, tz = "UTC")) 

archery <- hydrovu_utc %>%
  # mutate(DT = round_date(DT, "15 minutes")) %>%
  mutate(year = lubridate::year(DT)) %>%
  mutate(date = as_date(substr(DT, 1, 10))) %>%
  distinct(.keep_all = TRUE) %>%
  # Force time zones appropriately. MDT to UTC:
  mutate(DT = case_when(year == 2020 ~ DT + hours(6),
                        # MST to UTC:
                        year == 2019 ~ DT + hours(7),
                        TRUE ~ DT)) %>%
  mutate(sensor = "archery",
         comments = case_when(DT == "2019-09-30 03:45:00" ~ "sensor deployed",
                              DT == "2020-01-06 21:45:00" ~ "rental sensor battery dead",
                              DT == "2020-01-10 08:45:00" ~ "rental sensor battery replaced",
                              DT == "2020-04-04 18:00:00" ~ "rental sensor battery dead",
                              DT == "2020-04-17 11:15:00" ~ "rental sensor battery replaced",
                              DT == "2020-04-24 23:00:00" ~ "rental sensor stopped working?",
                              DT == "2020-04-25 15:45:00" ~ "rental sensor swapped with new sensor",
                              DT == "2021-08-17 09:00:00" ~ "sensor cleaned",
                              DT == "2021-08-17 09:15:00" ~ "sensor re-deployed",
                              DT == "2021-09-19 05:15:00" ~ "sensor stopped working - overheating?",
                              DT == "2021-09-25 09:30:00" ~ "sensor removed from field",
                              DT == "2021-10-04 09:00:00" ~ "sensor re-deployed",
                              DT == "2021-10-05 07:45:00" ~ "sensor checked, then re-deployed",
                              DT == "2021-12-04 09:00:00" ~ "sensor pulled for season"))

rm(hydrovu_utc)

old_raw <- bind_rows(rist, legacy, timberline, prospect, elc, archery) %>%
  mutate(sensor = ifelse(sensor == "rist", "tamasag", sensor))

# rm(archery, elc, legacy, prospect, rist, timberline)

# what we want to try to "recreate" as far as formatting:
# raw_example <- read_parquet("data/manual_data_verification/2024_cycle/hydro_vu_pull/raw/archery_virridy2025-01-01_0659.parquet", as_data_frame = TRUE)
```

**Standardization rules**

-   Long format by variable

-   Map old variable names → canonical `parameter` and `units`

-   Convert units (ft→m, mV→V, DO ppm→mg/L)

-   Common columns: `site`, `id`, `name`, `timestamp`, `parameter`, `value`, `units`

```{r}
raw_data_standardized <- old_raw %>%
  pivot_longer(-c(DT, year, date, sensor, comments), values_to = "value", names_to = "var") %>%
  filter(!var %in% c("Actual_Conductivity_µS_cm", "Salinity_PSU", "TDS_ppt", "Perc_O2_Sat", "TSS_mg_L",
                     "Pressure_PSI", "Elevation_m", "Depth_to_Water_ft", "Chla_µg_L")) %>%
  rename(site = sensor,
         timestamp = DT) %>%
  mutate(value = as.numeric(value),
         parameter = case_when(var == "Water_Temp_C" ~ "Temperature",
                               var == "pH" ~ "pH",
                               var == "ORP_mV" ~ "ORP",
                               var == "Specific_Conductivity_µS_cm" ~ "Specific Conductivity",
                               var == "DO_ppm" ~ "DO",
                               var ==  "Turbidity_NTU" ~ "Turbidity",
                               var == "Depth_ft" ~ "Depth",
                               var == "Chla_RFU" ~ "Chl-a Fluorescence"),
         # Original units
         original_unit = case_when(var == "Water_Temp_C" ~ "C",
                                   var == "pH" ~ "pH",
                                   var == "ORP_mV" ~ "mV",
                                   var == "Specific_Conductivity_µS_cm" ~ "µS/cm",
                                   var == "DO_ppm" ~ "ppm",
                                   var == "Turbidity_NTU" ~ "NTU",
                                   var == "Depth_ft" ~ "ft",
                                   var == "Chla_RFU" ~ "RFU"),
         # Target units
         units = case_when(var == "Water_Temp_C" ~ "C",
                           var == "pH" ~ "pH",
                           var == "ORP_mV" ~ "V",
                           var == "Specific_Conductivity_µS_cm" ~ "µS/cm",
                           var == "DO_ppm" ~ "mg/L",
                           var == "Turbidity_NTU" ~ "NTU",
                           var == "Depth_ft" ~ "m",
                           var == "Chla_RFU" ~ "RFU"),
         # Convert values to new units
         value = case_when(var == "DO_ppm" ~ value,  # ppm is equivalent to mg/L for dissolved oxygen
                           var == "Depth_ft" ~ value * 0.3048,  # feet to meters
                           var == "ORP_mV" ~ value / 1000,  # millivolts to volts
                           TRUE ~ value),
         id = site,
         name = site) %>%
  select(site, id, name, timestamp, parameter, value, units) %>%
  filter(!is.na(value)) %>%
  split(f = list(.$site), sep = "-")
```


## 4) Save site-year raw data (Parquet)

```{r}
# # Split by site and year
raw_data_site_year <- bind_rows(raw_data_standardized) %>%
  distinct(.keep_all = TRUE) %>%
  mutate(year = year(timestamp)) %>%
  split(f = list(.$site, .$year), sep = "_", drop = TRUE)

# Save files
iwalk(raw_data_site_year, ~ {
  latest_ts <- format(max(.x$timestamp, na.rm = TRUE), "%Y-%m-%d_%H%M")
  site_name <- str_split(.y, "_")[[1]][1]  # Extract just the site name (first part)
  filename <- paste0("data/manual_data_verification/", year(latest_ts), "_cycle/hydro_vu_pull/raw/", tolower(site_name), "_", latest_ts, ".parquet")
  .x %>% select(-year) %>% write_parquet(filename)
  cat("Saved:", filename, "\n")
})
```

## 5) Tidy, join field notes, summarize

There are, in fact, no field notes during this time frame. But we are running the data through the add_field_notes function to ensure all columns etc. are preserved for future QAQC steps.

```{r}
# tidy raw data (default 15-minute intervals)
tidy_data <-  raw_data_standardized %>%
  bind_rows() %>%
  mutate(DT_round = round_date(timestamp, "15 minutes")) %>%
  mutate(DT_join = as.character(paste(DT_round))) %>%
  fcw.qaqc::fix_site_names(., site_col = "site") %>%
  select(DT_round, DT_join, site, parameter, value, units) %>%
  distinct(.keep_all = TRUE) %>%
  modify_if(., is.numeric, ~ ifelse(is.nan(.x) | is.infinite(.x), NA, .x)) %>%
  split(f = list(.$site, .$parameter), sep = "-") %>%
  keep(~nrow(.) > 0) %>%
  future_map(~tidy_api_data(api_data = .), .progress = TRUE) %>%
  keep(~!is.null(.))

# add field notes to tidied data (there aren't any for these years)
combined_data <- tidy_data %>%
  future_map(~add_field_notes(df = ., notes = all_field_notes), .progress = TRUE)

# generate summary statistics
summarized_data <- combined_data %>%
  map(~generate_summary_statistics(.)) %>%
  # extra step to get rid of NaN and Inf values
  map(~ {modify_if(.x, is.numeric, ~ ifelse(is.nan(.x) | is.infinite(.x), NA, .x))})

```

Visualize all the data:
```{r}
plotz <- map(names(summarized_data), ~ {
  ggplot(data = summarized_data[[.x]]) +
    geom_line(aes(x = DT_round, y = mean), na.rm = TRUE) +
    labs(
      title = .x,
      x = "Date/Time",
      y = "Mean Value"
    ) +
    theme_minimal()
})

walk(plotz, print)
```


## 6) Single-sensor flags

```{r}
# Chunk the data for furrr
summarized_data_chunks <- split(1:length(summarized_data),
                                ceiling(seq_along(1:length(summarized_data))/10))
# Flag data...
# Single parameter flags

# Process the chunk in parallel
single_sensor_flags <- summarized_data %>%
  map(
    function(data) {
      flagged_data <- data %>%
        data.table(.) %>%
        # flag field visits
        # add_field_flag(df = .) %>%
        # flag missing data
        add_na_flag(df = .) %>%
        # flag DO noise
        find_do_noise(df = .) %>%
        # flag repeating values
        add_repeat_flag(df = .) %>%
        # find times when sonde was moved up/down in housing
        # add_depth_shift_flag(df = ., level_shift_table =  all_field_notes, post2024 = FALSE) %>%
        # find instances of sensor drift (FDOM, Chl-a, Turbidity only)
        add_drift_flag(df = .)
      
      if (unique(data$parameter) %in% names(sensor_thresholds)) {
        # flag instances outside the spec range
        flagged_data <- flagged_data %>%
          data.table(.) %>%
          add_spec_flag(df = ., spec_table = sensor_thresholds)
      }
      
      if (unique(data$parameter) %in% unique(season_thresholds$parameter)) {
        # flag instances outside the spec range
        flagged_data <- flagged_data %>%
          data.table(.) %>%
          add_seasonal_flag(df = ., threshold_table = season_thresholds)
      }
      
      flagged_data <- flagged_data %>%
        data.table(.)
      
      return(flagged_data)
    }
  )
```

## 7) Intrasensor flags (by site)

```{r}
# Intrasensor flags
intrasensor_flags <- single_sensor_flags %>%
  rbindlist(fill = TRUE) %>%
  split(by = "site")

# Chunk the data for furrr
intrasensor_data_chunks <- split(1:length(intrasensor_flags),
                                 ceiling(seq_along(1:length(intrasensor_flags))/3))

intrasensor_flags_list <- list()
for (chunk_idx in seq_along(intrasensor_data_chunks)) {
  message("\n=== Processing chunk ", chunk_idx, " of ", length(intrasensor_data_chunks), " ===")
  
  # Get the indices for this chunk
  indices <- intrasensor_data_chunks[[chunk_idx]]
  chunk_data <- intrasensor_flags[indices]
  # Process the chunk in parallel
  chunk_results <- chunk_data %>%
    map(
      function(data) {
        # A chunk is a site df
        flagged_data <- data %>%
          data.table() %>%
          # flag times when water was below freezing
          add_frozen_flag(.) %>%
          # overflagging correction. remove slope violation flag if it occurs concurrently
          # with temp or depth
          intersensor_check(.) %>%
          # add sonde burial. If DO is noise is long-term, likely burial:
          add_burial_flag(.) %>%
          # flag times when sonde was unsubmerged
          add_unsubmerged_flag(.)
        
        return(flagged_data)
      }, .progress = TRUE
    ) %>%
    rbindlist(fill = TRUE) %>%
    # lil' cleanup of flag column contents
    dplyr::mutate(flag = ifelse(flag == "", NA, flag)) %>%
    # transform back to site-parameter dfs
    split(f = list(.$site, .$parameter), sep = "-") %>%
    purrr::discard(~ nrow(.) == 0) %>%
    # Add in KNOWN instances of sensor malfunction
    map(~add_malfunction_flag(df = ., malfunction_records = sensor_malfunction_notes))
  
  # Add chunk to list
  intrasensor_flags_list <- c(intrasensor_flags_list, chunk_results)
  
  if (chunk_idx < length(intrasensor_data_chunks)) {
    message("Taking a short break before next chunk...")
    gc()
    Sys.sleep(0.1)
  }
}
```

## 9) Simple network consistency check

We apply a lightweight, order-aware comparison (upstream/downstream) to remove flags that are likely true environmental events observed across neighboring sites in a ±2-hour window. The site order defaults to CSU/FCW segments, with overrides for alternative networks.

```{r}
# Because we are pulling in all of the data for all of the sites, and the
# network check to do that is not applicable, here we make a custom net work check function
custom_network_check <- function(df, network = "csu", intrasensor_flags_arg = intrasensor_flags) {
  
  # Extract site and parameter name from dataframe
  site_name <- unique(na.omit(df$site))
  parameter_name <- unique(na.omit(df$parameter))
  
  # vector of sites in the order that they are in in the network
  if(network  %in% c("csu", "CSU", "fcw", "FCW")){
    
    sites_order <- c("bellvue",
                     "salyer",
                     "riverbend",
                     "cottonwood",
                     "elc",
                     "archery")
  } else {
    
    # Define site order based on spatial arrangement along river
    sites_order <-  c("joei",
                      "cbri",
                      "chd",
                      "pfal",
                      "pbd",
                      "pbr",
                      "pman",
                      "bellvue",
                      "salyer",
                      "udall",
                      "riverbend",
                      "cottonwood",
                      "elc",
                      "archery",
                      "riverbluffs")
    # Virridy sites
    if (site_name %in% c("cottonwood_virridy", "riverbend_virridy", "archery_virridy")){
      sites_order <- c("udall", "riverbend_virridy", "cottonwood_virridy", "archery_virridy", "riverbluffs")
    }
    # SFM network
    if (site_name %in% c("mtncampus", "sfm")){
      sites_order <- c("mtncampus", "sfm")
    }
    
    # Sites without a network
    if (site_name %in% c("lbea", "penn", "springcreek", "boxcreek")){
      return(df)
    }
    
  }
  
  # Find the index of current site in ordered list
  site_index <- which(sites_order == sites_order[grep(site_name, sites_order, ignore.case = TRUE)])
  
  # Create site-parameter identifier
  site_param <- paste0(site_name, "-", parameter_name)
  
  # Initialize empty dataframes for upstream/downstream sites
  upstr_site_df <- tibble::tibble(DT_round = NA) # Upstream sites
  dnstr_site_df <- tibble::tibble(DT_round = NA) # Downstream sites
  
  # Try to get upstream site data
  tryCatch({
    # Skip trying to find upstream sites for first site (Bellvue).
    if (site_index != 1){
      previous_site <- paste0(sites_order[site_index-1],"-",parameter_name)
      upstr_site_df <- intrasensor_flags_arg[[previous_site]] %>%
        dplyr::select(DT_round, site_up = site, flag_up = flag) %>%
        data.table::data.table()
    }
  },
  error = function(err) {
    message(paste0("No UPSTREAM data found for ", site_param, ". Expected at site '", previous_site, "'."))
  })
  
  # Try to get downstream site data
  tryCatch({
    # Skip trying to find downstream sites for last site (Riverbluffs).
    if (site_index != length(sites_order)){
      next_site <- paste0(sites_order[site_index+1],"-",parameter_name)
      dnstr_site_df <- intrasensor_flags_arg[[next_site]] %>%
        dplyr::select(DT_round, site_down = site, flag_down = flag) %>%
        data.table::data.table()
    }
  },
  error = function(err) {
    message(paste0("No DOWNSTREAM data found for ", site_param, ". Expected at site '", next_site, "'."))
  })
  
  # Join current site data with upstream and downstream data
  up_down_join <- df %>%
    dplyr::left_join(upstr_site_df, by = "DT_round") %>%
    dplyr::left_join(dnstr_site_df, by = "DT_round")
  
  # <<< Establish helper functions >>> ----
  
  # Function to add column if it doesn't exist
  add_column_if_not_exists <- function(df, column_name, default_value = NA) {
    if (!column_name %in% colnames(df)) {
      df <- df %>% dplyr::mutate(!!sym(column_name) := default_value)
    }
    return(df)
  }
  
  # Function to check if any flags exist in a time window
  check_2_hour_window_fail <- function(x) {
    sum(x) >= 1
  }
  
  # <<< Establish helper objects >>> ----
  
  # String object that is used to ignore flags that we do not want to remove.
  ignore_flags <- "drift|DO interference|repeat|sonde not employed|frozen|
  unsubmerged|missing data|site visit|sv window|sensor malfunction|burial|
  sensor biofouling|improper level cal|sonde moved"
  
  # Numeric object that determines the width for the rolling window check (2 hours)
  width_fun = 17
  
  # <<< Process flags based on upstream/downstream patterns >>> ----
  final_df <- up_down_join %>%
    # Add placeholder columns if joinging didn't provide them
    add_column_if_not_exists("flag_down") %>%
    add_column_if_not_exists("flag_up") %>%
    add_column_if_not_exists("site_down") %>%
    add_column_if_not_exists("site_up") %>%
    # Create binary indicator for upstream/downstream flags
    ## 0 = no relevant flags upstream/downstream
    ## 1 = at least one site has relevant flags
    dplyr::mutate(flag_binary = dplyr::if_else(
      (is.na(flag_up) | grepl(ignore_flags, flag_up)) &
        (is.na(flag_down) | grepl(ignore_flags, flag_down)), 0, 1
    )) %>%
    # Check for flags in 4-hour window (+/-2 hours around each point, 17 observations at 15-min intervals)
    dplyr::mutate(overlapping_flag = zoo::rollapply(flag_binary, width = width_fun, FUN = check_2_hour_window_fail, fill = NA, align = "center")) %>%
    add_column_if_not_exists(column_name = "auto_flag") %>%
    # If flag exists but is also present up/downstream, it likely represents a real event
    # In that case, remove the flag (set auto_flag to NA)
    dplyr::mutate(auto_flag = ifelse(!is.na(flag) & !grepl(ignore_flags, flag) &
                                       (overlapping_flag == TRUE & !is.na(overlapping_flag)), NA, flag)) %>%
    dplyr::select(-c(flag_up, flag_down, site_up, site_down, flag_binary, overlapping_flag))
  
  return(final_df)
}

# I really don't understand how network check would work in parallel, so I don't.
final_flags <- intrasensor_flags_list %>%
  purrr::map(~custom_network_check(df = ., intrasensor_flags_arg = intrasensor_flags_list)) %>%
  rbindlist(fill = TRUE) %>%
  tidy_flag_column() %>%
  split(f = list(.$site, .$parameter), sep = "_") %>%
  purrr::map(~add_suspect_flag(.)) %>%
  data.table::rbindlist(fill = TRUE)
```

## 10) Final post-processing & save outputs

We remove isolated one-off “suspect data” points and write per-site/parameter CSVs for the reporting cycle.

```{r}
v_final_flags <- final_flags%>%
  dplyr::mutate(auto_flag = ifelse(is.na(auto_flag), NA,
                                   ifelse(auto_flag == "suspect data" & is.na(lag(auto_flag, 1)) & is.na(lead(auto_flag, 1)), NA, auto_flag))) %>%
  dplyr::select(c("DT_round", "DT_join", "site", "parameter", "mean", "units", "n_obs", "spread", "auto_flag", "mal_flag", "sonde_moved","sonde_employed", "season", "last_site_visit")) %>%
  dplyr::mutate(auto_flag = ifelse(is.na(auto_flag), NA, ifelse(auto_flag == "", NA, auto_flag))) %>%
  mutate(year = year(DT_round)) %>%
  split(f = list(.$site, .$parameter, .$year), sep = "_") %>%
  keep(~nrow(.) > 0)

# Save files
iwalk(v_final_flags, ~ {
  site_name <- str_split(.y, "_")[[1]][1]  # Extract just the site name (first part)
  parm <- str_split(.y, "_")[[1]][2]
  year <- str_split(.y, "_")[[1]][3]
  filename <- paste0("data/manual_data_verification/", year, "_cycle/hydro_vu_pull/flagged/", tolower(site_name), "_", parm, ".csv")
  .x %>% select(-year) %>% write_csv(filename)
  cat("Saved:", filename, "\n")
})
```




